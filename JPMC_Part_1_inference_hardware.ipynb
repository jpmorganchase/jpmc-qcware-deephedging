{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4N54UWkze-O"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_peKxaSNo1T",
    "outputId": "14e52d8e-ce58-40ad-b71d-ae4be0fce23a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "  %pip install QuantLib\n",
    "  %pip install optax\n",
    "  %pip install qiskit\n",
    "  %pip install qcware\n",
    "\n",
    "  %pip install qcware-quasar\n",
    "  ! rm -rf deep-hedging\n",
    "  ! git clone https://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@github.com/SnehalRaj/jpmc-qcware-deephedging deep-hedging\n",
    "  ! cp -r deep-hedging/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FXSzgmKfN8-a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import config\n",
    "import numpy as np\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "from functools import partial \n",
    "import pickle \n",
    "\n",
    "from qnn import linear, ortho_linear, ortho_linear_noisy, ortho_linear_hardware\n",
    "from train import build_train_fn\n",
    "from models import simple_network, recurrent_network_hardware, lstm_network_hardware, attention_network\n",
    "from loss_metrics import entropy_loss\n",
    "from data import gen_paths\n",
    "from utils import train_test_split, get_batches, HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': \"{0:0.3f}\".format})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "A6TcMo24PAKh"
   },
   "source": [
    "# Data\n",
    "\n",
    "We begin by import the modules from the repository. Then to generate test data using geometric Brownian motion we use the `gen_paths()` function in `data.py`. \n",
    "\n",
    "Note: The data generation has been commented here because we pickled a particular test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOnoeMTmz7ZO",
    "outputId": "b6171e83-ceec-4116-e9fd-b8c6790c2fb6"
   },
   "outputs": [],
   "source": [
    "seed = 100\n",
    "key = jax.random.PRNGKey(seed)\n",
    "# No need to generate data as we load from disk\n",
    "# hps = HyperParams(S0=100,\n",
    "#                   n_steps=5,\n",
    "#                   n_paths=10000,\n",
    "#                   discrete_path=False,\n",
    "#                   strike_price=100,\n",
    "#                   epsilon=0.0,\n",
    "#                   sigma=0.2,\n",
    "#                   risk_free=0,\n",
    "#                   dividend=0,\n",
    "#                   model_type='simple',\n",
    "#                   layer_type='noisy_ortho',\n",
    "#                   n_features=8,\n",
    "#                   n_layers=1,\n",
    "#                   loss_param=1.0,\n",
    "#                   batch_size=4,\n",
    "#                   test_size=0.2,\n",
    "#                   optimizer='adam',\n",
    "#                   learning_rate=1E-3,\n",
    "#                   num_epochs=100\n",
    "#                   )\n",
    "\n",
    "\n",
    "# Data\n",
    "# S = gen_paths(hps)\n",
    "# [S_train, S_test] = train_test_split([S], test_size=0.2)\n",
    "# _, test_batches = get_batches(jnp.array(S_test[0]), batch_size=hps.batch_size)\n",
    "# test_batch = test_batches[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepHedgingBenchmark()\n",
    "\n",
    "This is a class to benchmark the performance of different models and layers for deep hedging. It has one main method __test_model() which is used to perform inference using the layers and models specified in the input. Note that choosing `hps.layer_type` to `ortho_linear_hardware()` runs inference on hardware backend using `run_circuit()` function defined above. \n",
    "\n",
    "#### Parameters\n",
    "- `key`: A random key value used for jax random splitting.\n",
    "- `eps`: A list of float values representing the hedge intervals to be used in training.\n",
    "- `layers`: A list of string values representing the layer types to be used in \n",
    "training. It should only contain values from `['linear', 'ortho', 'noisy_ortho', 'hardware_ortho']`.\n",
    "- `models`: A list of string values representing the model types to be used in training. It should only contain values from `['simple', 'recurrent', 'lstm', 'attention']`.\n",
    "\n",
    "#### Methods\n",
    "- `__test_model(hps,test_batch,backend_name, device_id, params_dir)`: A private method that tests the model. It takes in hyperparameters hps, a batch of paths `test_batch` to run inference on and `params_dir` which specifies where to load the saved model parameters from. The hyperparameters include layer_type, model_type, n_steps, epsilon, and num_epochs. It returns the testing loss.\n",
    "- `test(test_batch)`:  A method to test the model. It takes in `test_batch` which is the testing data and then output the necessary information about the model. It outputs the layer, the model architecture, the utility on these paths and the number of circuits executed. It also prints the output actions for each day and the Terminal PnL for each paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fVm_ZjYhHCQu"
   },
   "outputs": [],
   "source": [
    "from utils import load_params\n",
    "from hardware_utils import prepare_circuit, run_circuit\n",
    "class DeepHedgingBenchmark():\n",
    "  \"\"\"\n",
    "  Runs the benchmark with different models / layers\n",
    "  Input: test_batch above\n",
    "  test_batch has 8 datapoints\n",
    "  \"\"\"\n",
    "  def __init__(self, key, eps,  layers, models):\n",
    "      self.__key = key\n",
    "      self.__models = models\n",
    "      self.__layers = layers\n",
    "      self.__eps = eps\n",
    "      self.test_info = {layer:{str(eps):{} for eps in self.__eps} for layer in self.__layers}\n",
    "  def __test_model(self, hps, test_batch,backend_name, device_id, params_dir):\n",
    "    global_number_of_circuits_executed = 0\n",
    "    global_hardware_run_results_dict = {\n",
    "        'model_type' : hps.model_type,\n",
    "        'measurementRes' : None,\n",
    "        'epsilon' : hps.epsilon,\n",
    "        'backend_name' : None,\n",
    "        'layer_type' : hps.layer_type,\n",
    "        'batch_idx' : 0,\n",
    "    }\n",
    "    config.global_number_of_circuits_executed = global_number_of_circuits_executed  \n",
    "    config.global_hardware_run_results_dict = global_hardware_run_results_dict\n",
    "    if hps.layer_type in ['linear','linear_svb']:\n",
    "      layer_func = linear\n",
    "    elif hps.layer_type=='ortho':\n",
    "      layer_func = ortho_linear\n",
    "    elif hps.layer_type=='noisy_ortho':\n",
    "      layer_func = partial(ortho_linear_noisy,noise_scale=0.01)\n",
    "    elif hps.layer_type=='hardware_ortho':\n",
    "      # TODO want to run this on Quantinuum device \n",
    "      layer_func = partial(ortho_linear_hardware,prepare_circuit, partial(run_circuit,device_id = device_id,backend_name = backend_name) )\n",
    "\n",
    "    if hps.model_type == 'simple':\n",
    "      net = simple_network(hps=hps, layer_func=layer_func)\n",
    "    elif hps.model_type == 'recurrent':\n",
    "      net = recurrent_network_hardware(hps=hps, layer_func=layer_func)\n",
    "    elif hps.model_type == 'lstm':\n",
    "      net = lstm_network_hardware(hps=hps, layer_func=layer_func)\n",
    "    elif hps.model_type == 'attention':\n",
    "      net = attention_network(hps=hps, layer_func=layer_func)\n",
    "    \n",
    "    opt = optax.adam(1E-3)\n",
    "    key, init_key = jax.random.split(self.__key)\n",
    "    _, state, _ = net.init(init_key, (1, hps.n_steps, 1))\n",
    "    loss_metric = entropy_loss\n",
    "\n",
    "    # Training\n",
    "\n",
    "    train_fn, loss_fn = build_train_fn(hps, net, opt, loss_metric)\n",
    "\n",
    "    train_info = load_params(params_dir)\n",
    "    layer_type = \"noisy_ortho\" if hps.layer_type == 'hardware_ortho' else hps.layer_type\n",
    "    train_losses, params = train_info[layer_type][str(hps.epsilon)][hps.model_type]\n",
    "    loss, (state, wealths, deltas, outputs) = loss_fn(params, state, key, test_batch[...,None])\n",
    "    print(f'Model = {hps.model_type} | Layer = {hps.layer_type} | EPS = {hps.epsilon}| Loss = {loss} | #circs = {global_number_of_circuits_executed}')\n",
    "    # Display Deltas and Terminal PnL if batch size is small\n",
    "    if len(test_batch) < 10:\n",
    "      print(f'Deltas = {deltas[...,0]}')\n",
    "      print(f'Terminal PnL = {wealths.reshape(-1)}')\n",
    "    return loss\n",
    "  def test(self, inputs, backend_name, device_id, params_dir):\n",
    "    for model in self.__models:\n",
    "      for eps in self.__eps:\n",
    "        for layer in self.__layers:\n",
    "            hps = HyperParams(S0=100,\n",
    "                  discrete_path=True,\n",
    "                  strike_price=100,\n",
    "                  epsilon=eps,\n",
    "                  sigma=0.2,\n",
    "                  risk_free=0,\n",
    "                  dividend=0,\n",
    "                  model_type=model,\n",
    "                  layer_type=layer,\n",
    "                  n_features=16,\n",
    "                  n_layers=1,\n",
    "                  loss_param=1.0,\n",
    "                  batch_size=4,\n",
    "                  test_size=0.2,\n",
    "                  optimizer='adam',\n",
    "                  learning_rate=1E-3,\n",
    "                  num_epochs=100)\n",
    "            self.test_info[layer][str(eps)][model] = self.__test_model(hps, inputs, backend_name, device_id, params_dir)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Emulator Backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(test_batch, open('data/1103_test_batch_30_points.pickle', 'wb'))\n",
    "test_batch = pickle.load(open('data/1103_test_batch_30_points.pickle', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple, Recurrent, LSTM, and Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-emulator results:\n",
    "30 days, 32 paths\n",
    "\n",
    "```\n",
    "Model = simple | Layer = linear | EPS = 0.01| Loss = 5.014517784118652 | #circs = 0\n",
    "Model = simple | Layer = ortho | EPS = 0.01| Loss = 5.0412492752075195 | #circs = 0\n",
    "Model = simple | Layer = noisy_ortho | EPS = 0.01| Loss = 5.003141403198242 | #circs = 0\n",
    "Model = recurrent | Layer = linear | EPS = 0.01| Loss = 5.190323829650879 | #circs = 0\n",
    "Model = recurrent | Layer = ortho | EPS = 0.01| Loss = 5.0067362785339355 | #circs = 0\n",
    "Model = recurrent | Layer = noisy_ortho | EPS = 0.01| Loss = 4.838649272918701 | #circs = 0\n",
    "Model = lstm | Layer = linear | EPS = 0.01| Loss = 4.761508941650391 | #circs = 0\n",
    "Model = lstm | Layer = ortho | EPS = 0.01| Loss = 4.8098063468933105 | #circs = 0\n",
    "Model = lstm | Layer = noisy_ortho | EPS = 0.01| Loss = 4.798060417175293 | #circs = 0\n",
    "Model = attention | Layer = linear | EPS = 0.01| Loss = 4.776426315307617 | #circs = 0\n",
    "Model = attention | Layer = ortho | EPS = 0.01| Loss = 4.846531391143799 | #circs = 0\n",
    "Model = attention | Layer = noisy_ortho | EPS = 0.01| Loss = 4.843540191650391 | #circs = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precomputed counts from data/1128_part_1_simple_quantinuum_H1-1E_hardware_ortho_0.01_0.json\n",
      "Model = simple | Layer = hardware_ortho | EPS = 0.01| Loss = 5.155247688293457 | #circs = 0\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_0.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_1.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_2.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_3.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_4.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_5.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_6.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_7.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_8.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_9.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_10.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_11.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_12.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_13.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_14.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_15.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_16.json\n",
      "Using precomputed counts from data/1128_part_1_recurrent_quantinuum_H1-1E_hardware_ortho_0.01_17.json\n"
     ]
    }
   ],
   "source": [
    "LAYERS = ['hardware_ortho']\n",
    "EPS = [  0.01]\n",
    "MODELS = ['simple','recurrent','lstm','attention']\n",
    "\n",
    "dhb = DeepHedgingBenchmark(key=key,eps=EPS, layers=LAYERS, models=MODELS)\n",
    "dhb.test(test_batch, backend_name='quantinuum_H1-1E', device_id='1128_part_1', params_dir='params/params_all_models_16_qubits_30_days.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Backend "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(test_batch, open('data/1115_test_batch_4_points.pickle', 'wb'))\n",
    "test_batch = pickle.load(open('data/1115_test_batch_4_points.pickle', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical simulations results:\n",
    "5 days, 4 paths\n",
    "\n",
    "```\n",
    "Model = lstm | Layer = linear | EPS = 0.01| Loss = 2.1739442348480225 | #circs = 0\n",
    "Deltas = [[0.435 -0.002 -0.187 -0.182 -0.050 -0.014]\n",
    " [0.435 0.050 0.019 -0.001 -0.027 -0.476]\n",
    " [0.435 -0.015 -0.000 -0.085 -0.038 -0.296]\n",
    " [0.435 0.000 0.019 -0.001 -0.002 -0.451]]\n",
    "Terminal PnL = [-2.578 -1.225 -1.420 -2.671]\n",
    "Model = lstm | Layer = ortho | EPS = 0.01| Loss = 2.1762888431549072 | #circs = 0\n",
    "Deltas = [[0.435 0.001 -0.203 -0.165 -0.049 -0.019]\n",
    " [0.435 0.051 0.011 -0.006 -0.045 -0.447]\n",
    " [0.435 -0.013 0.004 -0.090 -0.025 -0.310]\n",
    " [0.435 0.000 0.010 0.001 -0.001 -0.446]]\n",
    "Terminal PnL = [-2.586 -1.194 -1.439 -2.671]\n",
    "Model = lstm | Layer = noisy_ortho | EPS = 0.01| Loss = 2.1889312267303467 | #circs = 0\n",
    "Deltas = [[0.437 0.004 -0.213 -0.158 -0.043 -0.027]\n",
    " [0.428 0.040 0.027 -0.009 -0.022 -0.463]\n",
    " [0.432 -0.019 0.000 -0.104 -0.018 -0.291]\n",
    " [0.435 -0.000 0.029 0.010 -0.001 -0.472]]\n",
    "Terminal PnL = [-2.620 -1.205 -1.435 -2.669]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-RtLEOMgIHt"
   },
   "outputs": [],
   "source": [
    "LAYERS = ['hardware_ortho']\n",
    "EPS = [  0.01]\n",
    "MODELS = ['lstm']\n",
    "\n",
    "dhb = DeepHedgingBenchmark(key=key,eps=EPS, layers=LAYERS, models=MODELS)\n",
    "dhb.test(test_batch, backend_name='quantinuum_H1-1', device_id='1115_device', params_dir='params/params_all_models_16_qubits_5_days.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-emulator results:\n",
    "5 days, 4 paths\n",
    "\n",
    "```\n",
    "Model = attention | Layer = linear | EPS = 0.01| Loss = 2.167210817337036 | #circs = 0\n",
    "Deltas = [[0.429 0.001 -0.225 -0.121 -0.064 -0.020]\n",
    " [0.429 0.038 0.030 -0.001 -0.020 -0.475]\n",
    " [0.429 -0.007 -0.004 -0.092 -0.013 -0.312]\n",
    " [0.429 0.001 0.028 0.002 -0.000 -0.459]]\n",
    "Terminal PnL = [-2.563 -1.219 -1.411 -2.673]\n",
    "Model = attention | Layer = ortho | EPS = 0.01| Loss = 2.1953773498535156 | #circs = 0\n",
    "Deltas = [[0.423 0.000 -0.167 -0.142 -0.095 -0.019]\n",
    " [0.423 0.062 0.023 -0.006 -0.056 -0.446]\n",
    " [0.423 -0.016 -0.014 -0.104 -0.040 -0.250]\n",
    " [0.423 0.005 0.031 0.003 -0.001 -0.462]]\n",
    "Terminal PnL = [-2.639 -1.242 -1.388 -2.672]\n",
    "Model = attention | Layer = noisy_ortho | EPS = 0.01| Loss = 2.4901790618896484 | #circs = 0\n",
    "Deltas = [[0.435 -0.014 -0.083 -0.074 -0.051 -0.213]\n",
    " [0.413 0.095 0.015 -0.004 -0.130 -0.388]\n",
    " [0.426 -0.034 0.012 -0.111 -0.047 -0.246]\n",
    " [0.421 -0.010 0.029 0.012 0.004 -0.456]]\n",
    "Terminal PnL = [-3.225 -1.270 -1.442 -2.728]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83HKjeblRioM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precomputed counts from data/1212_part1_2_attention_quantinuum_H1-1_hardware_ortho_0.01_0.json\n"
     ]
    }
   ],
   "source": [
    "LAYERS = ['hardware_ortho']\n",
    "EPS = [  0.01]\n",
    "MODELS = ['attention']\n",
    "\n",
    "dhb = DeepHedgingBenchmark(key=key,eps=EPS, layers=LAYERS, models=MODELS)\n",
    "dhb.test(test_batch, backend_name='quantinuum_H1-1', device_id='1212_part1_2', params_dir='params/params_all_models_16_qubits_5_days.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHOSr4XgzC4a",
    "outputId": "6cb557cf-99c7-4654-b4f0-6831cfb2c96a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_0.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_1.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_2.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_3.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_4.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_5.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_6.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_7.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_8.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_9.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_10.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_11.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_12.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_13.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_14.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_15.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_16.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_17.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_18.json\n",
      "Using precomputed counts from data/1115_device_lstm_quantinuum_H1-1_hardware_ortho_0.01_19.json\n",
      "Model = lstm | Layer = hardware_ortho | EPS = 0.01| Loss = 2.1947038173675537 | #circs = 0\n",
      "Deltas = [[0.439 -0.011 -0.186 -0.170 -0.047 -0.025]\n",
      " [0.436 0.043 0.037 0.003 -0.012 -0.506]\n",
      " [0.447 -0.003 0.007 -0.096 -0.011 -0.344]\n",
      " [0.435 0.004 0.038 0.007 -0.004 -0.481]]\n",
      "Terminal PnL = [-2.610 -1.284 -1.488 -2.658]\n"
     ]
    }
   ],
   "source": [
    "dhb.test(test_batch, backend_name='quantinuum_H1-1', device_id='1115_device', params_dir='params/params_all_models_16_qubits_5_days.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSgwaeMQIcjF",
    "outputId": "fcf0adc0-db39-497b-e3d4-d603b150300a"
   },
   "outputs": [],
   "source": [
    "# dhb.test(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esRqWWUQT4y4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jpmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad54ef31d854a95caa7f093c1b575c6633ad4c5ad56f723ed5e92cb8b22d7116"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
