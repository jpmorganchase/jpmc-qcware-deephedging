{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4N54UWkze-O"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_peKxaSNo1T",
    "outputId": "64d4f29c-f446-4b38-c6c4-ad9bf0a174b9"
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "  %pip install QuantLib\n",
    "  %pip install optax\n",
    "  %pip install qiskit\n",
    "  %pip install qcware\n",
    "\n",
    "  %pip install qcware-quasar\n",
    "  ! rm -rf deep-hedging\n",
    "  ! git clone https://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@github.com/SnehalRaj/jpmc-qcware-deephedging deep-hedging\n",
    "  ! cp -r deep-hedging/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FXSzgmKfN8-a"
   },
   "outputs": [],
   "source": [
    "import qiskit\n",
    "\n",
    "import quasar\n",
    "from qcware_transpile.translations.quasar.to_qiskit import translate, audit\n",
    "from qiskit.compiler import assemble\n",
    "import collections\n",
    "\n",
    "from qio import loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w9xYap3zhI6"
   },
   "source": [
    "# qnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K6NowCthzixV"
   },
   "outputs": [],
   "source": [
    "from typing import (Callable, List, Mapping, NamedTuple, Optional, Sequence,\n",
    "                    Tuple, Union)\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "from jax import lax\n",
    "from jax import numpy as jnp\n",
    "import itertools\n",
    "# Typing\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "Array = jnp.ndarray\n",
    "Shape = Sequence[int]\n",
    "Dtype = Union[jnp.float32, jnp.float64]\n",
    "PRNGKey = Array\n",
    "Params = Mapping[str, Mapping[str, jnp.ndarray]]\n",
    "State = Mapping[str, Mapping[str, jnp.ndarray]]\n",
    "InitializerFn = Callable[[PRNGKey, Shape, Dtype], Array]\n",
    "Initializer = Callable[..., InitializerFn]\n",
    "Module = Callable[..., InitializerFn]\n",
    "\n",
    "\n",
    "class ModuleFn(NamedTuple):\n",
    "    apply: Callable[..., Tuple[Array, State]]\n",
    "    init: Optional[Callable[..., Tuple[Params, State, Array]]] = None\n",
    "\n",
    "\n",
    "def add_scope_to_params(scope, params):\n",
    "    return dict((f\"{scope}/{key}\", array) for key, array in params.items())\n",
    "\n",
    "\n",
    "def get_params_by_scope(scope, params):\n",
    "    return dict((key[len(scope) + 1:], array) for key, array in params.items()\n",
    "                if key.startswith(scope + '/'))\n",
    "\n",
    "\n",
    "# Initializers\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def constant(val: float, ) -> InitializerFn:\n",
    "    \"\"\" Initialize with a constant value. \n",
    "\n",
    "    Args:\n",
    "        val: The value to initialize with.\n",
    "    \"\"\"\n",
    "    def init_fn(key, shape, dtype=jnp.float32):\n",
    "        return jnp.broadcast_to(val, shape).astype(dtype)\n",
    "\n",
    "    return init_fn\n",
    "\n",
    "\n",
    "def zeros() -> InitializerFn:\n",
    "    \"\"\" Initialize with zeros.\"\"\"\n",
    "    return constant(0.)\n",
    "\n",
    "\n",
    "def ones() -> InitializerFn:\n",
    "    \"\"\" Initialize with ones.\"\"\"\n",
    "    return constant(1.)\n",
    "\n",
    "\n",
    "def uniform(\n",
    "    minval: float = 0.,\n",
    "    maxval: float = 1.,\n",
    ") -> InitializerFn:\n",
    "    \"\"\" Initialize with a uniform distribution.\n",
    "\n",
    "    Args:\n",
    "        minval: The minimum value of the uniform distribution. \n",
    "        maxval: The maximum value of the uniform distribution.\n",
    "    \"\"\"\n",
    "    def init_fn(key, shape, dtype=jnp.float32):\n",
    "        return jax.random.uniform(key, shape, dtype, minval, maxval)\n",
    "\n",
    "    return init_fn\n",
    "\n",
    "\n",
    "def normal(\n",
    "    mean: float = 0.,\n",
    "    std: float = 1.,\n",
    ") -> InitializerFn:\n",
    "    \"\"\" Initialize with a normal distribution.\n",
    "\n",
    "    Args:\n",
    "        mean: The mean of the normal distribution.\n",
    "        std: The standard deviation of the normal distribution.\n",
    "    \"\"\"\n",
    "    def init_fn(key, shape, dtype=jnp.float32):\n",
    "        _mean = lax.convert_element_type(mean, dtype)\n",
    "        _std = lax.convert_element_type(std, dtype)\n",
    "        return _mean + _std * jax.random.normal(key, shape, dtype)\n",
    "\n",
    "    return init_fn\n",
    "\n",
    "\n",
    "def truncated_normal(\n",
    "    mean: float = 0.,\n",
    "    std: float = 1.,\n",
    ") -> InitializerFn:\n",
    "    \"\"\" Initialize with a truncated normal distribution.\n",
    "\n",
    "    Args:\n",
    "        mean: The mean of the truncated normal distribution.\n",
    "        std: The standard deviation of the truncated normal distribution.\n",
    "    \"\"\"\n",
    "    def init_fn(key, shape, dtype=jnp.float32):\n",
    "        _mean = lax.convert_element_type(mean, dtype)\n",
    "        _std = lax.convert_element_type(std, dtype)\n",
    "        return _mean + _std * jax.random.truncated_normal(\n",
    "            key, -2., 2., shape, dtype)\n",
    "\n",
    "    return init_fn\n",
    "\n",
    "\n",
    "# Modules\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def quax_wrapper(layer_fn):\n",
    "    \"\"\" Create a module from a quax layer. \"\"\"\n",
    "    def module(*args, **kwargs):\n",
    "        init_fn, apply_fn = layer_fn(*args, **kwargs)\n",
    "\n",
    "        def _apply_fn(params, state, key, inputs, **kwargs):\n",
    "            outputs = apply_fn(params, inputs, **kwargs)\n",
    "            return outputs, state\n",
    "\n",
    "        def _init_fn(key, inputs_shape):\n",
    "            shape, params = init_fn(key, inputs_shape)\n",
    "            state = None\n",
    "            return params, state, shape\n",
    "\n",
    "        return ModuleFn(_apply_fn, init=_init_fn)\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def haiku_wrapper(layer_fn):\n",
    "    \"\"\" Create a module from a Haiku layer. \"\"\"\n",
    "    def module(*args, **kwargs):\n",
    "        import haiku as hk\n",
    "        layer = hk.transform_with_state(layer_fn(*args, **kwargs))\n",
    "\n",
    "        def _apply_fn(params, state, key, inputs, **kwargs):\n",
    "            outputs, state = layer.apply(params, state, key, inputs, **kwargs)\n",
    "            return outputs, state\n",
    "\n",
    "        def _init_fn(key, inputs_shape):\n",
    "            params, state = layer.init(key, inputs_shape)\n",
    "            outputs, _ = layer.apply(params, state, key, inputs_shape,\n",
    "                                     **kwargs)\n",
    "            shape = outputs.shape\n",
    "            return params, state, shape\n",
    "\n",
    "        return ModuleFn(_apply_fn, init=_init_fn)\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def elementwise(elementwise_fn: Callable[[Array], Array], ) -> ModuleFn:\n",
    "    \"\"\" Create an elementwise layer from a JAX function. \n",
    "\n",
    "        Args:\n",
    "            elementwise_fn: The JAX function to apply to each element.\n",
    "    \"\"\"\n",
    "    return ModuleFn(apply=elementwise_fn)\n",
    "\n",
    "\n",
    "def linear(\n",
    "    n_features: int,\n",
    "    with_bias: bool = True,\n",
    "    w_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create a linear layer.\n",
    "\n",
    "    Args:\n",
    "        n_features: The number of features in the output.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        w_init: The initializer for the weights.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        outputs = jnp.dot(inputs, params['w'])\n",
    "\n",
    "        if with_bias:\n",
    "            outputs += params['b']\n",
    "        return outputs, None\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        params, state = {}, None\n",
    "        key, w_key, b_key = jax.random.split(key, 3)\n",
    "        w_init_ = w_init or truncated_normal(std=1. / inputs_shape[-1])\n",
    "        w_shape = (inputs_shape[-1], n_features)\n",
    "        params['w'] = w_init_(w_key, w_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        shape = inputs_shape[:-1] + (n_features, )\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)\n",
    "\n",
    "\n",
    "def layer_norm(\n",
    "    with_scale: bool = True,\n",
    "    with_bias: bool = True,\n",
    "    s_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create a normalization layer. \n",
    "\n",
    "    Args:\n",
    "        with_scale: Whether to use a scale parameter.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        s_init: The initializer for the scale.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def init_fn(key, inputs_shape):\n",
    "        params = {}\n",
    "        state = None\n",
    "        s_key, b_key = jax.random.split(key)\n",
    "        n_features = inputs_shape[-1]\n",
    "        if with_scale:\n",
    "            s_init_ = s_init or ones()\n",
    "            s_shape = (n_features, )\n",
    "            params['s'] = s_init_(s_key, s_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        return params, state, inputs_shape\n",
    "\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        mean = jnp.mean(inputs, axis=-1, keepdims=True)\n",
    "        var = jnp.var(inputs, axis=-1, keepdims=True) + 1e-5\n",
    "        outputs = params['s'] * (inputs - mean) / jnp.sqrt(var) + params['b']\n",
    "        return outputs, state\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)\n",
    "\n",
    "\n",
    "def sequential(*modules: List[ModuleFn], ) -> ModuleFn:\n",
    "    \"\"\" Create a sequential module from a list of modules.\n",
    "\n",
    "    Args:\n",
    "        modules: A list of modules.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        if key is not None:\n",
    "            key = jax.random.split(key, len(modules))\n",
    "        else:\n",
    "            key = len(modules) * [None]\n",
    "        new_state = dict(\n",
    "            ('layer_{}'.format(idx), None) for idx in range(len(modules)))\n",
    "        if state is None:\n",
    "            state = new_state\n",
    "        for idx, module in enumerate(modules):\n",
    "            if module.init is not None:\n",
    "                outputs, new_module_state = module.apply(\n",
    "                    params['layer_{}'.format(idx)],\n",
    "                    state['layer_{}'.format(idx)],\n",
    "                    key[idx],\n",
    "                    outputs,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                new_state['layer_{}'.format(idx)] = new_module_state\n",
    "            else:\n",
    "                outputs = module.apply(outputs)\n",
    "\n",
    "        state = new_state\n",
    "        return outputs, state\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        params = dict(\n",
    "            ('layer_{}'.format(idx), None) for idx in range(len(modules)))\n",
    "        state = dict(\n",
    "            ('layer_{}'.format(idx), None) for idx in range(len(modules)))\n",
    "        key = jax.random.split(key, len(modules))\n",
    "        shape = inputs_shape\n",
    "        for idx, module in enumerate(modules):\n",
    "            if module.init is not None:\n",
    "                module_params, module_state, shape = module.init(\n",
    "                    key[idx], shape)\n",
    "                params['layer_{}'.format(idx)] = module_params\n",
    "                state['layer_{}'.format(idx)] = module_state\n",
    "            else:\n",
    "                shape = module.apply(jnp.zeros(shape)).shape\n",
    "\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)\n",
    "\n",
    "\n",
    "def orthogonalize_weights(weights):\n",
    "    \"\"\"Take the current weight matrices for each layer, apply SVD decomposition on each one, \n",
    "    then transform the singular values, and finally recompose to make the weight matrix orthogonal.\n",
    "    U,s,V = SVD(W). then all singular values must be ~1. \n",
    "    Output : update the self.weights matrices. \n",
    "    Reference : Orthogonal Deep Neural Networks, K.Juia et al. 2019\"\"\"\n",
    "    epsilon = 0.5\n",
    "    U, s, V = jnp.linalg.svd(weights, full_matrices=False)\n",
    "    s = jnp.clip(s, 1/(1+epsilon), 1+epsilon)\n",
    "    # reform with the new singular values\n",
    "    weights = jnp.dot(U, jnp.dot(jnp.diag(s), V))\n",
    "    return weights\n",
    "\n",
    "\n",
    "def orthogonalize_params(params):\n",
    "    \"\"\"Take a dictionary of params and orthogonalize the weights\n",
    "    \"\"\"\n",
    "    for k1 in params.keys():\n",
    "        if params[k1] != None:\n",
    "            for k2 in params[k1].keys():\n",
    "                if k2.split('/')[-1] == 'w':\n",
    "                    params[k1][k2] = orthogonalize_weights(params[k1][k2])\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def _make_orthogonal_fn(rbs_idxs, size):\n",
    "    num_thetas = sum(map(len, rbs_idxs))\n",
    "    rbs_idxs = [list(map(list, rbs_idx)) for rbs_idx in rbs_idxs]\n",
    "    len_idxs = np.cumsum([0] + list(map(len, rbs_idxs)))\n",
    "\n",
    "    def _get_rbs_unitary(theta):\n",
    "        \"\"\" Returns the unitary matrix for a single RBS gate. \"\"\"\n",
    "        cos_theta, sin_theta = jnp.cos(theta), jnp.sin(theta)\n",
    "        unitary = jnp.array([\n",
    "            [cos_theta, sin_theta],\n",
    "            [-sin_theta, cos_theta],\n",
    "        ])\n",
    "        unitary = unitary.transpose(*[*range(2, unitary.ndim), 0, 1])\n",
    "        return unitary\n",
    "\n",
    "    def _get_rbs_unitary_grad(theta):\n",
    "        \"\"\" Returns the unitary matrix for a single RBS gate. \"\"\"\n",
    "        cos_theta, sin_theta = jnp.cos(theta), jnp.sin(theta)\n",
    "        unitary = jnp.array([\n",
    "            [-sin_theta, cos_theta],\n",
    "            [-cos_theta, -sin_theta],\n",
    "        ])\n",
    "        unitary = unitary.transpose(*[*range(2, unitary.ndim), 0, 1])\n",
    "        return unitary\n",
    "\n",
    "    @jax.custom_jvp\n",
    "    def _get_parallel_rbs_unitary(thetas):\n",
    "        \"\"\" Returns the unitary matrix for parallel RBS gates. \"\"\"\n",
    "        unitaries = []\n",
    "        for i, idxs in enumerate(rbs_idxs):\n",
    "            idxs = sum(idxs, [])\n",
    "            sub_thetas = thetas[len_idxs[i]:len_idxs[i + 1]]\n",
    "            rbs_blocks = _get_rbs_unitary(sub_thetas)\n",
    "            eye_block = jnp.eye(size - len(idxs), dtype=thetas.dtype)\n",
    "            permutation = idxs + [i for i in range(size) if i not in idxs]\n",
    "            permutation = np.argsort(permutation)\n",
    "            unitary = jax.scipy.linalg.block_diag(*rbs_blocks, eye_block)\n",
    "            unitary = unitary[permutation][:, permutation]\n",
    "            unitaries.append(unitary)\n",
    "        unitaries = jnp.stack(unitaries)\n",
    "        return unitaries\n",
    "\n",
    "    @_get_parallel_rbs_unitary.defjvp\n",
    "    def get_parallel_rbs_unitary_jvp(primals, tangents):\n",
    "        thetas, = primals\n",
    "        thetas_dot, = tangents\n",
    "        unitaries = []\n",
    "        unitaries_dot = []\n",
    "        for i, idxs in enumerate(rbs_idxs):\n",
    "            idxs = sum(idxs, [])\n",
    "            sub_thetas = thetas[len_idxs[i]:len_idxs[i + 1]]\n",
    "            sub_thetas_dot = thetas_dot[len_idxs[i]:len_idxs[i + 1]]\n",
    "            rbs_blocks = _get_rbs_unitary(sub_thetas)\n",
    "            rbs_blocks_grad = _get_rbs_unitary_grad(sub_thetas)\n",
    "            rbs_blocks_dot = sub_thetas_dot[..., None, None] * rbs_blocks_grad\n",
    "            eye_block = jnp.eye(size - len(idxs), dtype=thetas.dtype)\n",
    "            zero_block = jnp.zeros_like(eye_block)\n",
    "            permutation = idxs + [i for i in range(size) if i not in idxs]\n",
    "            permutation = np.argsort(permutation)\n",
    "            unitary = jax.scipy.linalg.block_diag(*rbs_blocks, eye_block)\n",
    "            unitary_dot = jax.scipy.linalg.block_diag(*rbs_blocks_dot,\n",
    "                                                      zero_block)\n",
    "            unitary = unitary[permutation][:, permutation]\n",
    "            unitary_dot = unitary_dot[permutation][:, permutation]\n",
    "            unitaries.append(unitary)\n",
    "            unitaries_dot.append(unitary_dot)\n",
    "        primal_out = jnp.stack(unitaries)\n",
    "        tangent_out = jnp.stack(unitaries_dot)\n",
    "        return primal_out, tangent_out\n",
    "\n",
    "    def orthogonal_fn(thetas, precision=None):\n",
    "        \"\"\" Returns the unitary matrix for a sequence of parallel RBS gates. \"\"\"\n",
    "        assert thetas.shape[0] == num_thetas, \"Wrong number of thetas.\"\n",
    "        unitaries = _get_parallel_rbs_unitary(thetas)\n",
    "        unitary = jnp.linalg.multi_dot(unitaries[::-1], precision=precision)\n",
    "        return unitary\n",
    "\n",
    "    return orthogonal_fn\n",
    "\n",
    "def make_general_orthogonal_fn(rbs_idxs, size):\n",
    "    num_thetas = sum(map(len, rbs_idxs))\n",
    "    rbs_idxs = [list(map(list, rbs_idx)) for rbs_idx in rbs_idxs]\n",
    "    len_idxs = np.cumsum([0] + list(map(len, rbs_idxs)))\n",
    "\n",
    "    def _get_rbs_unitary(theta):\n",
    "        \"\"\" Returns the unitary matrix for a single RBS gate. \"\"\"\n",
    "        cos_t, sin_t = jnp.cos(theta), jnp.sin(theta)\n",
    "        zeros = jnp.zeros_like(cos_t)\n",
    "        ones = jnp.ones_like(cos_t)\n",
    "        unitary = jnp.array([\n",
    "            [ones, zeros, zeros, zeros],\n",
    "            [zeros, cos_t, -sin_t, zeros],\n",
    "            [zeros, sin_t, cos_t, zeros],\n",
    "            [zeros, zeros, zeros, ones],\n",
    "        ])\n",
    "        unitary = unitary.transpose(*[*range(2, unitary.ndim), 0, 1])\n",
    "        return unitary\n",
    "\n",
    "    def _get_parallel_rbs_unitary(thetas):\n",
    "        \"\"\" Returns the unitary matrix for parallel RBS gates. \"\"\"\n",
    "        unitaries = []\n",
    "        num_qubits = size\n",
    "        map_qubits = [[0, 2**q] for q in range(num_qubits)]\n",
    "        for i, idxs in enumerate(rbs_idxs):\n",
    "            idxs = sum(idxs, [])\n",
    "            sub_thetas = thetas[len_idxs[i]:len_idxs[i + 1]]\n",
    "            rbs_blocks = _get_rbs_unitary(sub_thetas)\n",
    "            eye_block = jnp.eye(2**(size - len(idxs)) , dtype=thetas.dtype)\n",
    "            unitary =  tensordot_unitary([*rbs_blocks, eye_block])\n",
    "            unitary_qubits = idxs + [\n",
    "            q for q in range(num_qubits) if q not in idxs\n",
    "            ]\n",
    "            permutation = np.argsort([\n",
    "            sum(binary)\n",
    "            for binary in itertools.product(*(map_qubits[q]\n",
    "                                              for q in unitary_qubits)) \n",
    "            ])\n",
    "            unitary = unitary[permutation][:, permutation]\n",
    "            unitaries.append(unitary)\n",
    "        unitaries = jnp.stack(unitaries)\n",
    "        \n",
    "        return unitaries\n",
    "\n",
    "\n",
    "    def orthogonal_fn(thetas, precision=None):\n",
    "        \"\"\" Returns the unitary matrix for a sequence of parallel RBS gates. \"\"\"\n",
    "        assert thetas.shape[0] == num_thetas, \"Wrong number of thetas.\"\n",
    "        unitaries = _get_parallel_rbs_unitary(thetas)\n",
    "        unitary = jnp.linalg.multi_dot(unitaries[::-1], precision=precision)\n",
    "        return unitary\n",
    "\n",
    "    return orthogonal_fn\n",
    "\n",
    "def _get_pyramid_idxs(num_inputs, num_outputs):\n",
    "    num_max = max(num_inputs, num_outputs)\n",
    "    num_min = min(num_inputs, num_outputs)\n",
    "    if num_max == num_min:\n",
    "        num_min -= 1\n",
    "    end_idxs = np.concatenate(\n",
    "        [np.arange(1, num_max - 1), num_max - np.arange(1, num_min + 1)])\n",
    "    start_idxs = np.concatenate([\n",
    "        np.arange(end_idxs.shape[0] + num_min - num_max) % 2,\n",
    "        np.arange(num_max - num_min)\n",
    "    ])\n",
    "    if num_inputs < num_outputs:\n",
    "        start_idxs = start_idxs[::-1]\n",
    "        end_idxs = end_idxs[::-1]\n",
    "    rbs_idxs = [\n",
    "        np.arange(start_idxs[i], end_idxs[i] + 1).reshape(-1, 2)\n",
    "        for i in range(len(start_idxs))\n",
    "    ]\n",
    "    return rbs_idxs\n",
    "\n",
    "\n",
    "def _get_butterfly_idxs(num_inputs, num_outputs):\n",
    "    def _get_butterfly_idxs(n):\n",
    "        if n == 2:\n",
    "            return np.array([[[0, 1]]])\n",
    "        else:\n",
    "            rbs_idxs = _get_butterfly_idxs(n // 2)\n",
    "            first = np.concatenate([rbs_idxs, rbs_idxs + n // 2], 1)\n",
    "            last = np.arange(n).reshape(1, 2, n // 2).transpose(0, 2, 1)\n",
    "            rbs_idxs = np.concatenate([first, last], 0)\n",
    "            return rbs_idxs\n",
    "\n",
    "    circuit_dim = int(2**np.ceil(np.log2(max(num_inputs, num_outputs))))\n",
    "    rbs_idxs = _get_butterfly_idxs(circuit_dim)\n",
    "    if num_inputs < num_outputs:\n",
    "        rbs_idxs = rbs_idxs[::-1]\n",
    "    return rbs_idxs\n",
    "\n",
    "\n",
    "def ortho_linear(\n",
    "    n_features: int,\n",
    "    layout: Union[str, List[List[Tuple[int, int]]]] = 'butterfly',\n",
    "    normalize_inputs: bool = False,\n",
    "    normalize_outputs: bool = True,\n",
    "    normalize_stop_gradient: bool = True,\n",
    "    with_scale: bool = True,\n",
    "    with_bias: bool = True,\n",
    "    t_init: Optional[InitializerFn] = None,\n",
    "    s_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create an orthogonal layer from a layout of RBS gates.\n",
    "\n",
    "    Args:\n",
    "        n_features: The number of features in the output.\n",
    "        layout: The layout of the RBS gates.\n",
    "        normalize_inputs: Whether to normalize the inputs.\n",
    "        normalize_outputs: Whether to normalize the outputs.\n",
    "        normalize_stop_gradient: Whether to stop the gradient of the norm.\n",
    "        with_scale: Whether to use a scale parameter.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        t_init: The initializer for the angles.\n",
    "        s_init: The initializer for the scale.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = int(2**np.ceil(\n",
    "                np.log2(max(inputs.shape[-1], n_features))))\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            make_unitary = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = max(inputs.shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "            circuit_dim = max(\n",
    "                [max(idxs) for moment in layout for idxs in moment])\n",
    "        make_unitary = _make_orthogonal_fn(rbs_idxs[::-1], circuit_dim)\n",
    "        if normalize_inputs:\n",
    "            norm = jnp.linalg.norm(inputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            inputs /= norm\n",
    "        if inputs.shape[-1] < circuit_dim:\n",
    "            zeros = jnp.zeros(\n",
    "                (*inputs.shape[:-1], circuit_dim - inputs.shape[-1]), )\n",
    "            inputs = jnp.concatenate([zeros, inputs], axis=-1)\n",
    "        unitary = make_unitary(params['t'][::-1])\n",
    "        outputs = jnp.dot(inputs, unitary.T)[..., -n_features:]\n",
    "        if normalize_outputs:\n",
    "            norm = jnp.linalg.norm(outputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            outputs /= norm\n",
    "        if with_scale:\n",
    "            outputs *= params['s']\n",
    "        if with_bias:\n",
    "            outputs += params['b']\n",
    "        return outputs, None\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs_shape[-1], n_features)\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs_shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "        n_angles = sum(map(len, rbs_idxs))\n",
    "        params, state = {}, None\n",
    "        key, t_key, b_key, s_key = jax.random.split(key, 4)\n",
    "        t_init_ = t_init or uniform(-np.pi, np.pi)\n",
    "        t_shape = (n_angles, )\n",
    "        params['t'] = t_init_(t_key, t_shape)\n",
    "        if with_scale:\n",
    "            s_init_ = s_init or ones()\n",
    "            s_shape = (n_features, )\n",
    "            params['s'] = s_init_(s_key, s_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        shape = inputs_shape[:-1] + (n_features, )\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)\n",
    "\n",
    "\n",
    "\n",
    "def ortho_linear_noisy(\n",
    "    n_features: int,\n",
    "    noise_scale: float = 0.01,\n",
    "    layout: Union[str, List[List[Tuple[int, int]]]] = 'butterfly',\n",
    "    normalize_inputs: bool = True,\n",
    "    normalize_outputs: bool = True,\n",
    "    normalize_stop_gradient: bool = True,\n",
    "    with_scale: bool = True,\n",
    "    with_bias: bool = True,\n",
    "    t_init: Optional[InitializerFn] = None,\n",
    "    s_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create an orthogonal layer from a layout of RBS gates.\n",
    "    Args:\n",
    "        n_features: The number of features in the output.\n",
    "        layout: The layout of the RBS gates.\n",
    "        normalize_inputs: Whether to normalize the inputs.\n",
    "        normalize_outputs: Whether to normalize the outputs.\n",
    "        normalize_stop_gradient: Whether to stop the gradient of the norm.\n",
    "        with_scale: Whether to use a scale parameter.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        t_init: The initializer for the angles.\n",
    "        s_init: The initializer for the scale.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = int(2**np.ceil(\n",
    "                np.log2(max(inputs.shape[-1], n_features))))\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            make_unitary = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = max(inputs.shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "            circuit_dim = max(\n",
    "                [max(idxs) for moment in layout for idxs in moment])\n",
    "        make_unitary = _make_orthogonal_fn(rbs_idxs[::-1], circuit_dim)\n",
    "        if normalize_inputs:\n",
    "            norm = jnp.linalg.norm(inputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            inputs /= norm\n",
    "        if inputs.shape[-1] < circuit_dim:\n",
    "            zeros = jnp.zeros(\n",
    "                (*inputs.shape[:-1], circuit_dim - inputs.shape[-1]), )\n",
    "            inputs = jnp.concatenate([zeros, inputs], axis=-1)\n",
    "        unitary = make_unitary(params['t'][::-1])\n",
    "        outputs = jnp.dot(inputs, unitary.T)\n",
    "        if normalize_outputs:\n",
    "            norm = jnp.linalg.norm(outputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            outputs /= norm\n",
    "        outputs = jnp.einsum('...i,...i->...i',outputs,outputs)\n",
    "        outputs = outputs[..., -n_features:]\n",
    "        if with_scale:\n",
    "            outputs *= params['s']\n",
    "        if with_bias:\n",
    "            outputs += params['b']\n",
    "        key, _ = jax.random.split(key)\n",
    "        outputs += noise_scale*jax.random.normal(key, outputs.shape)\n",
    "        return outputs, state\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs_shape[-1], n_features)\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs_shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "        n_angles = sum(map(len, rbs_idxs))\n",
    "        params, state = {}, None\n",
    "        key, t_key, b_key, s_key = jax.random.split(key, 4)\n",
    "        t_init_ = t_init or uniform(-np.pi, np.pi)\n",
    "        t_shape = (n_angles, )\n",
    "        params['t'] = t_init_(t_key, t_shape)\n",
    "        if with_scale:\n",
    "            s_init_ = s_init or ones()\n",
    "            s_shape = (n_features, )\n",
    "            params['s'] = s_init_(s_key, s_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        shape = inputs_shape[:-1] + (n_features, )\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5IfE-WEzjqL"
   },
   "source": [
    "# Main\n",
    "\n",
    "## Circuit constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3g2FYe7qX20R"
   },
   "outputs": [],
   "source": [
    "# Global counter\n",
    "\n",
    "global_number_of_circuits_executed = 0\n",
    "\n",
    "# Global object keeping track of result\n",
    "# Used for pickling\n",
    "# Populated initially in DeepHedgingBenchmark().__test_model\n",
    "# and with run results in run_circuit\n",
    "# keeping track of batch_idx in scan (under \"Models\")\n",
    "\n",
    "global_hardware_run_results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1QEKbaJDOjXK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from qnn import _get_butterfly_idxs, _get_pyramid_idxs, _make_orthogonal_fn\n",
    "# fix for older versions of Qiskit\n",
    "if qiskit.__version__ <= '0.37.1':\n",
    "    import qiskit.providers.aer.noise as noise\n",
    "else:\n",
    "    import qiskit_aer.noise as noise\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def RBS_gate(theta):\n",
    "\n",
    "    def operator_function(parameters):\n",
    "            theta = parameters['theta']\n",
    "            c = np.cos(theta)\n",
    "            s = np.sin(theta)\n",
    "            return np.array([[1.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, c, s, 0.0],\n",
    "                             [0.0, -s, c, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 1.0]], dtype=np.complex128)\n",
    "\n",
    "    return quasar.Gate(\n",
    "        nqubit=2,\n",
    "        operator_function=operator_function,\n",
    "        parameters=collections.OrderedDict([('theta', theta)]),\n",
    "        name='RBS',\n",
    "        ascii_symbols=['B', 'S'])\n",
    "\n",
    "\n",
    "\n",
    "def prepare_circuit(input, params, loader_layout='parallel', layer_layout='butterfly'):\n",
    "    def _get_layer_circuit():\n",
    "      _params = np.array(params).astype('float')\n",
    "      if layer_layout == 'butterfly':\n",
    "        rbs_idxs = _get_butterfly_idxs(num_qubits, num_qubits)\n",
    "      elif layer_layout == 'pyramid':\n",
    "        rbs_idxs = _get_pyramid_idxs(num_qubits, num_qubits)\n",
    "      circuit_layer = quasar.Circuit()\n",
    "      idx_angle = 0\n",
    "      for gates_per_timestep in rbs_idxs[::-1]:\n",
    "        for gate in gates_per_timestep:\n",
    "          circuit_layer.add_gate(quasar.Gate.RBS(theta=-_params[::-1][idx_angle]), tuple(gate))\n",
    "          idx_angle+=1\n",
    "      return circuit_layer\n",
    "    \n",
    "    num_qubits = len(input)\n",
    "    loader_circuit = loader(np.array(input),mode=loader_layout,initial=True,controlled=False)\n",
    "    layer_circuit = _get_layer_circuit()\n",
    "    circuit = quasar.Circuit.join_in_time([loader_circuit, layer_circuit])\n",
    "    # Translate from qcware-quasar to qiskit\n",
    "    qiskit_circuit = translate(circuit)\n",
    "    \n",
    "    # qiskit_circuit.save_statevector()    \n",
    "\n",
    "    qiskit_circuit = qiskit.transpile(qiskit_circuit, optimization_level=3)\n",
    "    c = qiskit.ClassicalRegister(num_qubits)\n",
    "    qiskit_circuit.add_register(c)\n",
    "    qiskit_circuit.barrier()\n",
    "    qiskit_circuit.measure(qubit=range(num_qubits),cbit=c)\n",
    "    return qiskit_circuit\n",
    "\n",
    "def counter_to_dict(c):\n",
    "    \"\"\"Converts counter returned by pytket get_counts function\n",
    "    to dictionary returned by qiskit\n",
    "    canonical use:\n",
    "    >>> result = backend.get_result(handle)\n",
    "    >>> counts = result.get_counts(basis=BasisOrder.dlo)\n",
    "    >>> counts_qiskit = counter_to_dict(counts)\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for k, v in c.items():\n",
    "        d[''.join(str(x) for x in k)] = int(v)\n",
    "    return d\n",
    "\n",
    "def run_circuit(circs,circuit_dim, backend_name = 'qiskit_noisy'):\n",
    "    \"\"\"\n",
    "    backend name accepted \n",
    "    \"\"\"\n",
    "    global global_number_of_circuits_executed\n",
    "    global global_hardware_run_results_dict\n",
    "    input_size = circuit_dim\n",
    "    results = np.zeros((len(circs), input_size))\n",
    "    \n",
    "    global_number_of_circuits_executed += len(circs)\n",
    "    num_measurements = 1000\n",
    "    \n",
    "    if \"qiskit\" in backend_name:\n",
    "        backend = qiskit.Aer.get_backend('qasm_simulator')\n",
    "        if backend_name == 'qiskit_noiseless':\n",
    "            measurement = qiskit.execute(circs, backend, shots=num_measurements)\n",
    "        elif backend_name == 'qiskit_noisy': \n",
    "            # Error probabilities\n",
    "            prob_1 = 0.001  # 1-qubit gate\n",
    "            prob_2 = 0.01   # 2-qubit gate\n",
    "            # Dylan's tunes error probabilities\n",
    "            # prob_1 = 0  # 1-qubit gate\n",
    "            # prob_2 = 3.5e-3   # 2-qubit gate\n",
    "\n",
    "            # Depolarizing quantum errors\n",
    "            error_1 = noise.depolarizing_error(prob_1, 1)\n",
    "            error_2 = noise.depolarizing_error(prob_2, 2)\n",
    "\n",
    "            # Add errors to noise model\n",
    "            noise_model = noise.NoiseModel()\n",
    "            noise_model.add_all_qubit_quantum_error(error_1, ['h', 'x', 'ry'])\n",
    "            noise_model.add_all_qubit_quantum_error(error_2, ['cz'])\n",
    "\n",
    "            # Get basis gates from noise model\n",
    "            basis_gates = noise_model.basis_gates\n",
    "            measurement = qiskit.execute(circs, backend,basis_gates=basis_gates, noise_mode=noise_model, shots=num_measurements)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected backend name {backend_name}\")\n",
    "        all_counts = measurement.result().get_counts()\n",
    "    elif \"quantinuum\" in backend_name:\n",
    "        # From docs: \"Batches cannot exceed the maximum limit of 500 H-System Quantum Credits (HQCs) total\"\n",
    "        # Therefore batching is more or less useless on quantinuum\n",
    "        from pytket.extensions.qiskit import qiskit_to_tk\n",
    "        from pytket.circuit import BasisOrder\n",
    "        from pytket.extensions.quantinuum import QuantinuumBackend\n",
    "    \n",
    "        outpath_stem = \"_\".join([\n",
    "            \"1128_part_1\",\n",
    "            global_hardware_run_results_dict['model_type'],\n",
    "            backend_name,\n",
    "            global_hardware_run_results_dict['layer_type'],\n",
    "            str(global_hardware_run_results_dict['epsilon']),\n",
    "            str(global_hardware_run_results_dict['batch_idx']),\n",
    "        ])\n",
    "        \n",
    "        outpath_result_final = f\"data/{outpath_stem}.json\"\n",
    "        outpath_handles = f\"data/handles_{outpath_stem}.pickle\"\n",
    "        \n",
    "        if Path(outpath_result_final).exists():\n",
    "            # if precomputed results already present on disk, simply load\n",
    "            print(f\"Using precomputed counts from {outpath_result_final}\")\n",
    "            all_counts = json.load(open(outpath_result_final, \"r\"))['all_counts']\n",
    "        else:\n",
    "            if backend_name == \"quantinuum_H1-2E\":\n",
    "                backend = QuantinuumBackend(device_name=\"H1-2E\")\n",
    "            elif backend_name == \"quantinuum_H1-2\":\n",
    "                backend = QuantinuumBackend(device_name=\"H1-2\")\n",
    "            elif backend_name == \"quantinuum_H1-1E\":\n",
    "                backend = QuantinuumBackend(device_name=\"H1-1E\")\n",
    "            elif backend_name == \"quantinuum_H1-1\":\n",
    "                backend = QuantinuumBackend(device_name=\"H1-1\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown Quantinuum backend: {backend_name}\")\n",
    "            if Path(outpath_handles).exists():\n",
    "                # if circuits already submitted, simply load from disk\n",
    "                print(f\"Using pickled handles from {outpath_handles}\")\n",
    "                handles = pickle.load(open(outpath_handles, \"rb\"))\n",
    "            else:\n",
    "                # otherwise, submit circuits and pickle handles\n",
    "                circs_tk = [qiskit_to_tk(circ) for circ in circs]\n",
    "                for idx, circ in enumerate(circs_tk):\n",
    "                    circ.name = f'{outpath_stem}_{idx+1}_of_{len(circs)}'\n",
    "                compiled_circuits = backend.get_compiled_circuits(circs_tk, optimisation_level=2)\n",
    "                handles = backend.process_circuits(compiled_circuits, n_shots=num_measurements)\n",
    "                pickle.dump(handles, open(outpath_handles, \"wb\"))\n",
    "                print(f\"Dumped handles to {outpath_handles}\")\n",
    "            # retrieve results from handles\n",
    "            result_list = []\n",
    "            \n",
    "            with tqdm(total=len(handles), desc='#jobs finished') as pbar:\n",
    "                for handle in handles:\n",
    "                    while True:\n",
    "                        status = backend.circuit_status(handle).status\n",
    "                        if status.name == 'COMPLETED':\n",
    "                            result = backend.get_result(handle)\n",
    "                            result_list.append(copy.deepcopy(result))\n",
    "                            pbar.update(1)\n",
    "                            break\n",
    "                        else:\n",
    "                            assert status.name in ['QUEUED', 'RUNNING'] \n",
    "                        time.sleep(1)\n",
    "            global_hardware_run_results_dict['result_list'] = [x.to_dict() for x in result_list]\n",
    "            # convert from tket counts format to qiskit\n",
    "            all_counts = [\n",
    "                counter_to_dict(\n",
    "                    result.get_counts(basis=BasisOrder.dlo)\n",
    "                ) for result in result_list\n",
    "            ]\n",
    "            global_hardware_run_results_dict['all_counts'] = all_counts\n",
    "            # dump result on disk\n",
    "            json.dump(global_hardware_run_results_dict, open(outpath_result_final, \"w\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected backend name {backend_name}\")\n",
    "        \n",
    "    global_hardware_run_results_dict['batch_idx'] += 1    \n",
    "    # Post processing\n",
    "    # Discard bitstrings that do not correspond to unary encoding (not Hamming weight 1)\n",
    "    # We build a dictionary with all unary bitstrings and only add counts corresponding to unary bitstrings\n",
    "    # Note: f\"{2**i:0{input_size}b}\" converts 2**i to its binary string representation.\n",
    "    for j in range(len(circs)):\n",
    "        measurementRes = all_counts[j]\n",
    "        num_postselected = 0\n",
    "        filtered_counts = {f\"{2**i:0{input_size}b}\":0 for i in range(input_size)}\n",
    "        for bitstring, count in measurementRes.items():\n",
    "            if sum([int(x) for x in bitstring]) != 1:\n",
    "                continue\n",
    "            filtered_counts[bitstring] += count\n",
    "            num_postselected+= count\n",
    "        results[j] = [filtered_counts[k]/num_postselected for k in sorted(filtered_counts)][::-1]    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__w2M8wQskTn"
   },
   "source": [
    "## Definition of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TqFtMKbVhkR9"
   },
   "outputs": [],
   "source": [
    "\n",
    "def ortho_linear_hardware(\n",
    "    n_features: int,\n",
    "    layout: Union[str, List[List[Tuple[int, int]]]] = 'butterfly',\n",
    "    normalize_inputs: bool = True,\n",
    "    normalize_outputs: bool = True,\n",
    "    normalize_stop_gradient: bool = True,\n",
    "    with_scale: bool = True,\n",
    "    with_bias: bool = True,\n",
    "    t_init: Optional[InitializerFn] = None,\n",
    "    s_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create an orthogonal layer from a layout of RBS gates.\n",
    "    Args:\n",
    "        n_features: The number of features in the output.\n",
    "        layout: The layout of the RBS gates.\n",
    "        normalize_inputs: Whether to normalize the inputs.\n",
    "        normalize_outputs: Whether to normalize the outputs.\n",
    "        normalize_stop_gradient: Whether to stop the gradient of the norm.\n",
    "        with_scale: Whether to use a scale parameter.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        t_init: The initializer for the angles.\n",
    "        s_init: The initializer for the scale.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        # Step 1: preprocess the inputs\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = int(2**np.ceil(\n",
    "                np.log2(max(inputs.shape[-1], n_features))))\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            make_unitary = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = max(inputs.shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "            circuit_dim = max(\n",
    "                [max(idxs) for moment in layout for idxs in moment])\n",
    "        if normalize_inputs:\n",
    "            norm = jnp.linalg.norm(inputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            inputs /= norm\n",
    "        if inputs.shape[-1] < circuit_dim:\n",
    "            zeros = jnp.zeros(\n",
    "                (*inputs.shape[:-1], circuit_dim - inputs.shape[-1]), )\n",
    "            inputs = jnp.concatenate([zeros, inputs], axis=-1)\n",
    "        # Step 2: generate the circuits\n",
    "        circs = []\n",
    "        out_shape = inputs.shape[:-1]+(n_features,)\n",
    "        for input in inputs.reshape(-1,circuit_dim):\n",
    "            circs.append(prepare_circuit(input,params['t']))\n",
    "        # run circuits and truncate to desired number of outputs\n",
    "        outputs = jnp.array(run_circuit(circs, circuit_dim))[..., -n_features:]\n",
    "        \n",
    "        outputs = outputs.reshape(out_shape)\n",
    "        # unitary = make_unitary(params['t'])\n",
    "        # outputs = jnp.dot(inputs, unitary.T)[..., -n_features:]\n",
    "        # outputs = inputs\n",
    "        if with_scale:\n",
    "            outputs *= params['s']\n",
    "        if with_bias:\n",
    "            outputs += params['b']\n",
    "        return outputs, state\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs_shape[-1], n_features)\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs_shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "        n_angles = sum(map(len, rbs_idxs))\n",
    "        params, state = {}, None\n",
    "        key, t_key, b_key, s_key = jax.random.split(key, 4)\n",
    "        t_init_ = t_init or uniform(-np.pi, np.pi)\n",
    "        t_shape = (n_angles, )\n",
    "        params['t'] = t_init_(t_key, t_shape)\n",
    "        if with_scale:\n",
    "            s_init_ = s_init or ones()\n",
    "            s_shape = (n_features, )\n",
    "            params['s'] = s_init_(s_key, s_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        shape = inputs_shape[:-1] + (n_features, )\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6TcMo24PAKh"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YjoX6Hojg1Ev"
   },
   "outputs": [],
   "source": [
    "from typing import Any, TypeVar\n",
    "import itertools\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import qnn\n",
    "from qnn import ModuleFn, elementwise, linear, sequential, make_general_orthogonal_fn, _get_butterfly_idxs, _get_pyramid_idxs \n",
    "\n",
    "\n",
    "relu = elementwise(jax.nn.relu)\n",
    "gelu = elementwise(jax.nn.gelu)\n",
    "log_softmax = elementwise(jax.nn.log_softmax)\n",
    "sigmoid = elementwise(jax.nn.sigmoid)\n",
    "\n",
    "def scan(f, init, xs, length=None):\n",
    "  if xs is None:\n",
    "    xs = [None] * length\n",
    "  carry = init\n",
    "  ys = []\n",
    "  for x in xs:\n",
    "    carry, y = f(carry, x)\n",
    "    ys.append(y)\n",
    "  return carry, jnp.stack(ys)\n",
    "\n",
    "def recurrent_network(hps, layer_func: ModuleFn = linear, **kwargs) -> ModuleFn:\n",
    "    \"\"\" Create a Recurrent Network.\n",
    "    Args:\n",
    "        n_features: The number of features.\n",
    "        n_layers: The number of layers.\n",
    "        layer_func: The type of layers to use.\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessing = [linear(hps.n_features), sigmoid]\n",
    "    features = hps.n_layers * [layer_func(hps.n_features), relu]\n",
    "    postprocessing = [linear(1), sigmoid]\n",
    "    layers = preprocessing + features + postprocessing\n",
    "    net = sequential(*layers)\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        params = net.init(\n",
    "            key, (inputs_shape[0], inputs_shape[1], 2*inputs_shape[2]))[0]\n",
    "        return params, None, inputs_shape\n",
    "\n",
    "    def apply_fn(params, state, key, inputs):\n",
    "        def cell_fn(prev_outputs, inputs):\n",
    "            inp = inputs[None, ...]\n",
    "            inp = jnp.concatenate([prev_outputs, inp], axis=-1)\n",
    "            delta = net.apply(params, None, key, inp)[0]\n",
    "            # print(f'inputs shape = {inp.shape} deltas shape = {delta.shape}')\n",
    "            return delta, delta\n",
    "\n",
    "        prev_state = jnp.zeros((1, inputs.shape[0], inputs.shape[-1]))\n",
    "        inputs = inputs.transpose(1, 0, 2)\n",
    "        _, outputs = scan(cell_fn, prev_state, inputs)\n",
    "        outputs = jnp.squeeze(outputs, 1)\n",
    "        outputs = outputs.transpose(1, 0, 2)\n",
    "        return outputs, state\n",
    "    return qnn.ModuleFn(apply_fn, init_fn)\n",
    "\n",
    "\n",
    "def lstm_cell(hps,  layer_func: ModuleFn = linear, **kwargs) -> ModuleFn:\n",
    "    \"\"\" Create an LSTM Cell.\n",
    "    Args:\n",
    "        n_features: The number of features.\n",
    "        layer_func: The type of layers to use.\n",
    "    \"\"\"\n",
    "\n",
    "    _linear = layer_func(n_features=int(hps.n_features/2), with_bias=True)\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        keys = jax.random.split(key, num=4)\n",
    "        params = {}\n",
    "        layer_idx = ['i', 'g', 'f', 'o']\n",
    "        _shape = (inputs_shape[0], inputs_shape[1], 2*inputs_shape[2])\n",
    "        _init_params = {}\n",
    "        for i, id in enumerate(layer_idx):\n",
    "            _init_params[id] = _linear.init(keys[i],  _shape)[0]\n",
    "            params.update(qnn.add_scope_to_params(id, _init_params[id]))\n",
    "        return params, None, inputs_shape\n",
    "\n",
    "    def apply_fn(params, state, key, inputs):\n",
    "        def cell_fn(prev_state, inputs):\n",
    "            prev_hidden, prev_cell = prev_state\n",
    "            x_and_h = jnp.concatenate([inputs, prev_hidden], axis=-1)\n",
    "            layer_idx = ['i', 'g', 'f', 'o']\n",
    "            _apply_params = {}\n",
    "            for i, id in enumerate(layer_idx):\n",
    "                _apply_params[id] = qnn.get_params_by_scope(id, params)\n",
    "            i = _linear.apply(_apply_params['i'], None, key, x_and_h)[0]\n",
    "            g = _linear.apply(_apply_params['g'], None, key, x_and_h)[0]\n",
    "            f = _linear.apply(_apply_params['f'], None, key, x_and_h)[0]\n",
    "            o = _linear.apply(_apply_params['o'], None, key, x_and_h)[0]\n",
    "            # i = input, g = cell_gate, f = forget_gate, o = output_gate\n",
    "            f = jax.nn.sigmoid(f + 1)\n",
    "            c = f * prev_cell + jax.nn.sigmoid(i) * jnp.tanh(g)\n",
    "            h = jax.nn.sigmoid(o) * jnp.tanh(c)\n",
    "            return jnp.stack([h, c], axis=0), h\n",
    "\n",
    "        prev_state = jnp.zeros((2, inputs.shape[0], inputs.shape[-1]))\n",
    "        inputs = inputs.transpose(1, 0, 2)\n",
    "        _, outputs = scan(cell_fn, prev_state, inputs)\n",
    "        outputs = outputs.transpose(1, 0, 2)\n",
    "        return outputs, state\n",
    "    return qnn.ModuleFn(apply_fn, init_fn)\n",
    "\n",
    "\n",
    "def lstm_network(hps, layer_func: ModuleFn = linear, **kwargs) -> ModuleFn:\n",
    "    \"\"\" Create an LSTM Network.\n",
    "    Args:\n",
    "        n_features: The number of features.\n",
    "        layer_func: The type of layers to use.\n",
    "    \"\"\"\n",
    "    preprocessing = [linear( int(hps.n_features/2) ), sigmoid]\n",
    "    features = [lstm_cell(hps=hps, layer_func=layer_func)]\n",
    "    postprocessing = [linear(1), sigmoid]\n",
    "    layers = preprocessing + features + postprocessing\n",
    "    net = sequential(*layers)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NOnoeMTmz7ZO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# from models import simple_network, attention_network\n",
    "from qnn import linear\n",
    "from train import build_train_fn\n",
    "from qnn import ortho_linear, ortho_linear_noisy\n",
    "from models import simple_network, attention_network\n",
    "from loss_metrics import entropy_loss\n",
    "from data import gen_paths\n",
    "from utils import train_test_split, get_batches, HyperParams\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "from functools import partial \n",
    "from utils import HyperParams\n",
    "seed = 100\n",
    "key = jax.random.PRNGKey(seed)\n",
    "## No need to reconstruct data as we are loading from disk\n",
    "# hps = HyperParams(S0=100,\n",
    "#                   n_steps=5,\n",
    "#                   n_paths=10000,\n",
    "#                   discrete_path=False,\n",
    "#                   strike_price=100,\n",
    "#                   epsilon=0.0,\n",
    "#                   sigma=0.2,\n",
    "#                   risk_free=0,\n",
    "#                   dividend=0,\n",
    "#                   model_type='simple',\n",
    "#                   layer_type='noisy_ortho',\n",
    "#                   n_features=8,\n",
    "#                   n_layers=1,\n",
    "#                   loss_param=1.0,\n",
    "#                   batch_size=4,\n",
    "#                   test_size=0.2,\n",
    "#                   optimizer='adam',\n",
    "#                   learning_rate=1E-3,\n",
    "#                   num_epochs=100\n",
    "#                   )\n",
    "\n",
    "\n",
    "\n",
    "# # Data\n",
    "# S = gen_paths(hps)\n",
    "# [S_train, S_test] = train_test_split([S], test_size=0.2)\n",
    "# _, test_batches = get_batches(jnp.array(S_test[0]), batch_size=hps.batch_size)\n",
    "# test_batch = test_batches[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fVm_ZjYhHCQu"
   },
   "outputs": [],
   "source": [
    "from utils import load_params\n",
    "class DeepHedgingBenchmark():\n",
    "  \"\"\"\n",
    "  Runs the benchmark with different models / layers\n",
    "  Input: test_batch above\n",
    "  test_batch has 8 datapoints\n",
    "  \"\"\"\n",
    "  def __init__(self, key, eps,  layers, models):\n",
    "      self.__key = key\n",
    "      self.__models = models\n",
    "      self.__layers = layers\n",
    "      self.__eps = eps\n",
    "      self.test_info = {layer:{str(eps):{} for eps in self.__eps} for layer in self.__layers}\n",
    "  def __test_model(self, hps, test_batch, save_dir = 'params/params_all_models_16_qubits_30_days.pkl'):\n",
    "    # set up global objects for pickling circuit execution results\n",
    "    global global_number_of_circuits_executed\n",
    "    global global_hardware_run_results_dict\n",
    "    global_number_of_circuits_executed = 0\n",
    "    global_hardware_run_results_dict = {\n",
    "        'model_type' : hps.model_type,\n",
    "        'measurementRes' : None,\n",
    "        'epsilon' : hps.epsilon,\n",
    "        'backend_name' : None,\n",
    "        'layer_type' : hps.layer_type,\n",
    "        'batch_idx' : 0,\n",
    "    }\n",
    "    if hps.layer_type in ['linear','linear_svb']:\n",
    "      layer_func = linear\n",
    "    elif hps.layer_type=='ortho':\n",
    "      layer_func = ortho_linear\n",
    "    elif hps.layer_type=='noisy_ortho':\n",
    "      layer_func = partial(ortho_linear_noisy,noise_scale=0.01)\n",
    "    elif hps.layer_type=='hardware_ortho':\n",
    "      # TODO want to run this on Quantinuum device \n",
    "      layer_func = ortho_linear_hardware\n",
    "\n",
    "    if hps.model_type == 'simple':\n",
    "      net = simple_network(hps=hps, layer_func=layer_func)\n",
    "    elif hps.model_type == 'recurrent':\n",
    "      net = recurrent_network(hps=hps, layer_func=layer_func)\n",
    "    elif hps.model_type == 'lstm':\n",
    "      net = lstm_network(hps=hps, layer_func=layer_func)\n",
    "    elif hps.model_type == 'attention':\n",
    "      net = attention_network(hps=hps, layer_func=layer_func)\n",
    "    \n",
    "    opt = optax.adam(1E-3)\n",
    "    key, init_key = jax.random.split(self.__key)\n",
    "    _, state, _ = net.init(init_key, (1, hps.n_steps, 1))\n",
    "    loss_metric = entropy_loss\n",
    "\n",
    "    # Training\n",
    "\n",
    "    train_fn, loss_fn = build_train_fn(hps, net, opt, loss_metric)\n",
    "\n",
    "    train_info = load_params(save_dir)\n",
    "    layer_type = \"noisy_ortho\" if hps.layer_type == 'hardware_ortho' else hps.layer_type\n",
    "    train_losses, params = train_info[layer_type][str(hps.epsilon)][hps.model_type]\n",
    "    loss, (state, wealths, deltas, outputs) = loss_fn(params, state, key, test_batch[...,None])\n",
    "    print(f'Model = {hps.model_type} | Layer = {hps.layer_type} | EPS = {hps.epsilon}| Loss = {loss} | #circs = {global_number_of_circuits_executed}')\n",
    "    return loss\n",
    "  def test(self, inputs):\n",
    "    for model in self.__models:\n",
    "      for eps in self.__eps:\n",
    "        for layer in self.__layers:\n",
    "            hps = HyperParams(S0=100,\n",
    "                  n_steps=30,\n",
    "                  n_paths=120000,\n",
    "                  discrete_path=True,\n",
    "                  strike_price=100,\n",
    "                  epsilon=eps,\n",
    "                  sigma=0.2,\n",
    "                  risk_free=0,\n",
    "                  dividend=0,\n",
    "                  model_type=model,\n",
    "                  layer_type=layer,\n",
    "                  n_features=16,\n",
    "                  n_layers=1,\n",
    "                  loss_param=1.0,\n",
    "                  batch_size=32,\n",
    "                  test_size=0.2,\n",
    "                  optimizer='adam',\n",
    "                  learning_rate=1E-3,\n",
    "                  num_epochs=100)\n",
    "            self.test_info[layer][str(eps)][model] = self.__test_model(hps, inputs)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T-RtLEOMgIHt"
   },
   "outputs": [],
   "source": [
    "seed = 100\n",
    "key = jax.random.PRNGKey(seed)\n",
    "\n",
    "LAYERS = ['hardware_ortho']\n",
    "EPS = [  0.01]\n",
    "MODELS = ['simple','recurrent','lstm','attention']\n",
    "\n",
    "# LAYERS = ['linear','ortho','noisy_ortho','hardware_ortho']\n",
    "# EPS = [ 0.01]\n",
    "# MODELS = ['lstm']\n",
    "\n",
    "# test only\n",
    "\n",
    "# LAYERS = ['hardware_ortho']\n",
    "# EPS = [ 0.01]\n",
    "# MODELS = ['simple','recurrent','lstm','attention']\n",
    "\n",
    "dhb = DeepHedgingBenchmark(key=key,eps=EPS, layers=LAYERS, models=MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RxuPah3oKIZF"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': \"{0:0.3f}\".format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "83HKjeblRioM"
   },
   "outputs": [],
   "source": [
    "# Fixing test_batch as suggested by Snehal\n",
    "# pickle.dump(test_batch, open('data/1103_test_batch_30_points.pickle', 'wb'))\n",
    "test_batch = pickle.load(open('data/1103_test_batch_30_points.pickle', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3V7QYQWlnlHy"
   },
   "source": [
    "Non-emulator results:\n",
    "30 days, 32 paths\n",
    "\n",
    "To get non-emulator results, please set `backend_name=qiskit_noisy` in the `run_circuit()` function above.\n",
    "```\n",
    "Model = simple | Layer = linear | EPS = 0.01| Loss = 5.014517784118652 | #circs = 0\n",
    "Model = simple | Layer = ortho | EPS = 0.01| Loss = 5.0412492752075195 | #circs = 0\n",
    "Model = simple | Layer = noisy_ortho | EPS = 0.01| Loss = 5.003141403198242 | #circs = 0\n",
    "Model = recurrent | Layer = linear | EPS = 0.01| Loss = 5.190323829650879 | #circs = 0\n",
    "Model = recurrent | Layer = ortho | EPS = 0.01| Loss = 5.0067362785339355 | #circs = 0\n",
    "Model = recurrent | Layer = noisy_ortho | EPS = 0.01| Loss = 4.838649272918701 | #circs = 0\n",
    "Model = lstm | Layer = linear | EPS = 0.01| Loss = 4.761508941650391 | #circs = 0\n",
    "Model = lstm | Layer = ortho | EPS = 0.01| Loss = 4.8098063468933105 | #circs = 0\n",
    "Model = lstm | Layer = noisy_ortho | EPS = 0.01| Loss = 4.798060417175293 | #circs = 0\n",
    "Model = attention | Layer = linear | EPS = 0.01| Loss = 4.776426315307617 | #circs = 0\n",
    "Model = attention | Layer = ortho | EPS = 0.01| Loss = 4.846531391143799 | #circs = 0\n",
    "Model = attention | Layer = noisy_ortho | EPS = 0.01| Loss = 4.843540191650391 | #circs = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PHOSr4XgzC4a"
   },
   "outputs": [
    {
     "ename": "QiskitError",
     "evalue": "'Keyboard interrupt in parallel_map.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/SnehalRaj-jpmc/jpmc-check/lib/python3.8/site-packages/qiskit/tools/parallel.py:164\u001b[0m, in \u001b[0;36mparallel_map\u001b[0;34m(task, values, task_args, task_kwargs, num_processes)\u001b[0m\n\u001b[1;32m    163\u001b[0m     param \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m value: (task, value, task_args, task_kwargs), values)\n\u001b[0;32m--> 164\u001b[0m     future \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mmap(_task_wrapper, param)\n\u001b[1;32m    166\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(future)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.9/lib/python3.8/concurrent/futures/_base.py:644\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshutdown(wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    645\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.9/lib/python3.8/concurrent/futures/process.py:686\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> 686\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_queue_management_thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m    687\u001b[0m \u001b[39m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[39m# objects that use file descriptors.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.9/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1012\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.9/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[39melif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1028\u001b[0m     lock\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mQiskitError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dhb\u001b[39m.\u001b[39;49mtest(test_batch)\n",
      "Cell \u001b[0;32mIn [9], line 84\u001b[0m, in \u001b[0;36mDeepHedgingBenchmark.test\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__layers:\n\u001b[1;32m     65\u001b[0m     hps \u001b[39m=\u001b[39m HyperParams(S0\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     66\u001b[0m           n_steps\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,\n\u001b[1;32m     67\u001b[0m           n_paths\u001b[39m=\u001b[39m\u001b[39m120000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m           learning_rate\u001b[39m=\u001b[39m\u001b[39m1E-3\u001b[39m,\n\u001b[1;32m     83\u001b[0m           num_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_info[layer][\u001b[39mstr\u001b[39m(eps)][model] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__test_model(hps, inputs)\n",
      "Cell \u001b[0;32mIn [9], line 58\u001b[0m, in \u001b[0;36mDeepHedgingBenchmark.__test_model\u001b[0;34m(self, hps, test_batch, save_dir)\u001b[0m\n\u001b[1;32m     56\u001b[0m layer_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnoisy_ortho\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m hps\u001b[39m.\u001b[39mlayer_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhardware_ortho\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m hps\u001b[39m.\u001b[39mlayer_type\n\u001b[1;32m     57\u001b[0m train_losses, params \u001b[39m=\u001b[39m train_info[layer_type][\u001b[39mstr\u001b[39m(hps\u001b[39m.\u001b[39mepsilon)][hps\u001b[39m.\u001b[39mmodel_type]\n\u001b[0;32m---> 58\u001b[0m loss, (state, wealths, deltas, outputs) \u001b[39m=\u001b[39m loss_fn(params, state, key, test_batch[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m,\u001b[39mNone\u001b[39;49;00m])\n\u001b[1;32m     59\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mModel = \u001b[39m\u001b[39m{\u001b[39;00mhps\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m | Layer = \u001b[39m\u001b[39m{\u001b[39;00mhps\u001b[39m.\u001b[39mlayer_type\u001b[39m}\u001b[39;00m\u001b[39m | EPS = \u001b[39m\u001b[39m{\u001b[39;00mhps\u001b[39m.\u001b[39mepsilon\u001b[39m}\u001b[39;00m\u001b[39m| Loss = \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m | #circs = \u001b[39m\u001b[39m{\u001b[39;00mglobal_number_of_circuits_executed\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/train.py:39\u001b[0m, in \u001b[0;36mbuild_train_fn.<locals>.loss_fn\u001b[0;34m(params, state, key, inputs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     I \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mlog(inputs \u001b[39m/\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m outputs, state \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mapply(params, state, key, I[:,:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,:])\n\u001b[1;32m     40\u001b[0m outputs \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mconcatenate( \n\u001b[1;32m     41\u001b[0m     (\n\u001b[1;32m     42\u001b[0m         outputs,\n\u001b[1;32m     43\u001b[0m      jnp\u001b[39m.\u001b[39mzeros_like(outputs[:, [\u001b[39m0\u001b[39m], :])\n\u001b[1;32m     44\u001b[0m ), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m deltas \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mconcatenate(\n\u001b[1;32m     47\u001b[0m     (\n\u001b[1;32m     48\u001b[0m         outputs[:, [\u001b[39m0\u001b[39m], :],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     52\u001b[0m )\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/models.py:48\u001b[0m, in \u001b[0;36msimple_network.<locals>.apply_fn\u001b[0;34m(params, state, key, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m T \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([T]\u001b[39m*\u001b[39mbatch_size)\n\u001b[1;32m     47\u001b[0m inputs \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mconcatenate((inputs, T), axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m outputs, state \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mapply(params, state, key, inputs)\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m outputs, state\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/qnn.py:264\u001b[0m, in \u001b[0;36msequential.<locals>.apply_fn\u001b[0;34m(params, state, key, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mfor\u001b[39;00m idx, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(modules):\n\u001b[1;32m    263\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39minit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m         outputs, new_module_state \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    265\u001b[0m             params[\u001b[39m'\u001b[39;49m\u001b[39mlayer_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(idx)],\n\u001b[1;32m    266\u001b[0m             state[\u001b[39m'\u001b[39;49m\u001b[39mlayer_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(idx)],\n\u001b[1;32m    267\u001b[0m             key[idx],\n\u001b[1;32m    268\u001b[0m             outputs,\n\u001b[1;32m    269\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    270\u001b[0m         )\n\u001b[1;32m    271\u001b[0m         new_state[\u001b[39m'\u001b[39m\u001b[39mlayer_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(idx)] \u001b[39m=\u001b[39m new_module_state\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [6], line 55\u001b[0m, in \u001b[0;36mortho_linear_hardware.<locals>.apply_fn\u001b[0;34m(params, state, key, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     circs\u001b[39m.\u001b[39mappend(prepare_circuit(\u001b[39minput\u001b[39m,params[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     54\u001b[0m \u001b[39m# run circuits and truncate to desired number of outputs\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m outputs \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray(run_circuit(circs, circuit_dim))[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m-\u001b[39mn_features:]\n\u001b[1;32m     57\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mreshape(out_shape)\n\u001b[1;32m     58\u001b[0m \u001b[39m# unitary = make_unitary(params['t'])\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# outputs = jnp.dot(inputs, unitary.T)[..., -n_features:]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m# outputs = inputs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [14], line 116\u001b[0m, in \u001b[0;36mrun_circuit\u001b[0;34m(circs, circuit_dim, backend_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[39m# Get basis gates from noise model\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     basis_gates \u001b[39m=\u001b[39m noise_model\u001b[39m.\u001b[39mbasis_gates\n\u001b[0;32m--> 116\u001b[0m     measurement \u001b[39m=\u001b[39m qiskit\u001b[39m.\u001b[39;49mexecute(circs, backend,basis_gates\u001b[39m=\u001b[39;49mbasis_gates, noise_mode\u001b[39m=\u001b[39;49mnoise_model, shots\u001b[39m=\u001b[39;49mnum_measurements)\n\u001b[1;32m    117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected backend name \u001b[39m\u001b[39m{\u001b[39;00mbackend_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/jpmc-check/lib/python3.8/site-packages/qiskit/execute_function.py:375\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(experiments, backend, basis_gates, coupling_map, backend_properties, initial_layout, seed_transpiler, optimization_level, pass_manager, qobj_id, qobj_header, shots, memory, max_credits, seed_simulator, default_qubit_los, default_meas_los, qubit_lo_range, meas_lo_range, schedule_los, meas_level, meas_return, memory_slots, memory_slot_size, rep_time, rep_delay, parameter_binds, schedule_circuit, inst_map, meas_map, scheduling_method, init_qubits, **run_config)\u001b[0m\n\u001b[1;32m    373\u001b[0m     run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mparameter_binds\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m parameter_binds\n\u001b[1;32m    374\u001b[0m run_kwargs\u001b[39m.\u001b[39mupdate(run_config)\n\u001b[0;32m--> 375\u001b[0m job \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49mrun(experiments, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrun_kwargs)\n\u001b[1;32m    376\u001b[0m end_time \u001b[39m=\u001b[39m time()\n\u001b[1;32m    377\u001b[0m _log_submission_time(start_time, end_time)\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/jpmc-check/lib/python3.8/site-packages/qiskit/utils/deprecation.py:28\u001b[0m, in \u001b[0;36mdeprecate_arguments.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[1;32m     27\u001b[0m     _rename_kwargs(func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, kwargs, kwarg_map, category)\n\u001b[0;32m---> 28\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/jpmc-check/lib/python3.8/site-packages/qiskit_aer/backends/aerbackend.py:187\u001b[0m, in \u001b[0;36mAerBackend.run\u001b[0;34m(self, circuits, validate, parameter_binds, **run_options)\u001b[0m\n\u001b[1;32m    185\u001b[0m     qobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assemble(circuits, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrun_options)\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     qobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assemble(circuits, parameter_binds\u001b[39m=\u001b[39;49mparameter_binds, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrun_options)\n\u001b[1;32m    189\u001b[0m \u001b[39m# Optional validation\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m validate:\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/jpmc-check/lib/python3.8/site-packages/qiskit_aer/backends/aerbackend.py:400\u001b[0m, in \u001b[0;36mAerBackend._assemble\u001b[0;34m(self, circuits, parameter_binds, **run_options)\u001b[0m\n\u001b[1;32m    398\u001b[0m             qobj \u001b[39m=\u001b[39m qobj_tmp\n\u001b[1;32m    399\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     qobj \u001b[39m=\u001b[39m assemble(circuits, backend\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    402\u001b[0m \u001b[39m# Add optypes to qobj\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39m# We convert to strings to avoid pybinding of types\u001b[39;00m\n\u001b[1;32m    404\u001b[0m qobj\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moptypes \u001b[39m=\u001b[39m [\n\u001b[1;32m    405\u001b[0m     \u001b[39mset\u001b[39m(i\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m optype) \u001b[39mif\u001b[39;00m optype \u001b[39melse\u001b[39;00m \u001b[39mset\u001b[39m()\n\u001b[1;32m    406\u001b[0m     \u001b[39mfor\u001b[39;00m optype \u001b[39min\u001b[39;00m optypes]\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/jpmc-check/lib/python3.8/site-packages/qiskit/compiler/assembler.py:208\u001b[0m, in \u001b[0;36massemble\u001b[0;34m(experiments, backend, qobj_id, qobj_header, shots, memory, max_credits, seed_simulator, qubit_lo_freq, meas_lo_freq, qubit_lo_range, meas_lo_range, schedule_los, meas_level, meas_return, meas_map, memory_slot_size, rep_time, rep_delay, parameter_binds, parametric_pulses, init_qubits, **run_config)\u001b[0m\n\u001b[1;32m    206\u001b[0m     end_time \u001b[39m=\u001b[39m time()\n\u001b[1;32m    207\u001b[0m     _log_assembly_time(start_time, end_time)\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m assemble_circuits(\n\u001b[1;32m    209\u001b[0m         circuits\u001b[39m=\u001b[39;49mbound_experiments,\n\u001b[1;32m    210\u001b[0m         qobj_id\u001b[39m=\u001b[39;49mqobj_id,\n\u001b[1;32m    211\u001b[0m         qobj_header\u001b[39m=\u001b[39;49mqobj_header,\n\u001b[1;32m    212\u001b[0m         run_config\u001b[39m=\u001b[39;49mrun_config,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    215\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(exp, (ScheduleBlock, Schedule, Instruction)) \u001b[39mfor\u001b[39;00m exp \u001b[39min\u001b[39;00m experiments):\n\u001b[1;32m    216\u001b[0m     run_config \u001b[39m=\u001b[39m _parse_pulse_args(\n\u001b[1;32m    217\u001b[0m         backend,\n\u001b[1;32m    218\u001b[0m         meas_level,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrun_config_common_dict,\n\u001b[1;32m    225\u001b[0m     )\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/jpmc-check/lib/python3.8/site-packages/qiskit/assembler/assemble_circuits.py:317\u001b[0m, in \u001b[0;36massemble_circuits\u001b[0;34m(circuits, run_config, qobj_id, qobj_header)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39m\"\"\"Assembles a list of circuits into a qobj that can be run on the backend.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \n\u001b[1;32m    307\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m    The qobj to be run on the backends\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m# assemble the circuit experiments\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m experiments_and_pulse_libs \u001b[39m=\u001b[39m parallel_map(_assemble_circuit, circuits, [run_config])\n\u001b[1;32m    318\u001b[0m experiments \u001b[39m=\u001b[39m []\n\u001b[1;32m    319\u001b[0m pulse_library \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/SnehalRaj-jpmc/jpmc-check/lib/python3.8/site-packages/qiskit/tools/parallel.py:173\u001b[0m, in \u001b[0;36mparallel_map\u001b[0;34m(task, values, task_args, task_kwargs, num_processes)\u001b[0m\n\u001b[1;32m    171\u001b[0m     Publisher()\u001b[39m.\u001b[39mpublish(\u001b[39m\"\u001b[39m\u001b[39mterra.parallel.finish\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mQISKIT_IN_PARALLEL\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFALSE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 173\u001b[0m     \u001b[39mraise\u001b[39;00m QiskitError(\u001b[39m\"\u001b[39m\u001b[39mKeyboard interrupt in parallel_map.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m# Otherwise just reset parallel flag and error\u001b[39;00m\n\u001b[1;32m    175\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mQISKIT_IN_PARALLEL\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFALSE\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mQiskitError\u001b[0m: 'Keyboard interrupt in parallel_map.'"
     ]
    }
   ],
   "source": [
    "dhb.test(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSgwaeMQIcjF"
   },
   "outputs": [],
   "source": [
    "# dhb.test(test_batch)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jpmc-check",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aeebd5b777ec7291cb611112c8ba5d8721451214eea05e9ccddb5913c09817ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
