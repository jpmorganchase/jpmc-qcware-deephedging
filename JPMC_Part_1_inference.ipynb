{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4N54UWkze-O"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_peKxaSNo1T",
    "outputId": "3d16d3b3-e86e-49a6-887c-a2c5d61f14db"
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "  %pip install QuantLib\n",
    "  %pip install optax\n",
    "  %pip install qiskit\n",
    "  %pip install qcware\n",
    "\n",
    "  %pip install qcware-quasar\n",
    "  ! rm -rf deep-hedging\n",
    "  ! git clone https://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@github.com/SnehalRaj/jpmc-qcware-deephedging deep-hedging\n",
    "  ! cp -r deep-hedging/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FXSzgmKfN8-a"
   },
   "outputs": [],
   "source": [
    "import qiskit\n",
    "\n",
    "import quasar\n",
    "from qcware_transpile.translations.quasar.to_qiskit import translate\n",
    "from qiskit.compiler import assemble\n",
    "import collections\n",
    "\n",
    "from qio import loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w9xYap3zhI6"
   },
   "source": [
    "# qnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K6NowCthzixV"
   },
   "outputs": [],
   "source": [
    "from typing import (Callable, List, Mapping, NamedTuple, Optional, Sequence,\n",
    "                    Tuple, Union)\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "from jax import lax\n",
    "from jax import numpy as jnp\n",
    "import itertools\n",
    "# Typing\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "Array = jnp.ndarray\n",
    "Shape = Sequence[int]\n",
    "Dtype = Union[jnp.float32, jnp.float64]\n",
    "PRNGKey = Array\n",
    "Params = Mapping[str, Mapping[str, jnp.ndarray]]\n",
    "State = Mapping[str, Mapping[str, jnp.ndarray]]\n",
    "InitializerFn = Callable[[PRNGKey, Shape, Dtype], Array]\n",
    "Initializer = Callable[..., InitializerFn]\n",
    "Module = Callable[..., InitializerFn]\n",
    "\n",
    "\n",
    "class ModuleFn(NamedTuple):\n",
    "    apply: Callable[..., Tuple[Array, State]]\n",
    "    init: Optional[Callable[..., Tuple[Params, State, Array]]] = None\n",
    "\n",
    "\n",
    "def add_scope_to_params(scope, params):\n",
    "    return dict((f\"{scope}/{key}\", array) for key, array in params.items())\n",
    "\n",
    "\n",
    "def get_params_by_scope(scope, params):\n",
    "    return dict((key[len(scope) + 1:], array) for key, array in params.items()\n",
    "                if key.startswith(scope + '/'))\n",
    "\n",
    "\n",
    "# Initializers\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def constant(val: float, ) -> InitializerFn:\n",
    "    \"\"\" Initialize with a constant value. \n",
    "\n",
    "    Args:\n",
    "        val: The value to initialize with.\n",
    "    \"\"\"\n",
    "    def init_fn(key, shape, dtype=jnp.float32):\n",
    "        return jnp.broadcast_to(val, shape).astype(dtype)\n",
    "\n",
    "    return init_fn\n",
    "\n",
    "\n",
    "def zeros() -> InitializerFn:\n",
    "    \"\"\" Initialize with zeros.\"\"\"\n",
    "    return constant(0.)\n",
    "\n",
    "\n",
    "def ones() -> InitializerFn:\n",
    "    \"\"\" Initialize with ones.\"\"\"\n",
    "    return constant(1.)\n",
    "\n",
    "\n",
    "def uniform(\n",
    "    minval: float = 0.,\n",
    "    maxval: float = 1.,\n",
    ") -> InitializerFn:\n",
    "    \"\"\" Initialize with a uniform distribution.\n",
    "\n",
    "    Args:\n",
    "        minval: The minimum value of the uniform distribution. \n",
    "        maxval: The maximum value of the uniform distribution.\n",
    "    \"\"\"\n",
    "    def init_fn(key, shape, dtype=jnp.float32):\n",
    "        return jax.random.uniform(key, shape, dtype, minval, maxval)\n",
    "\n",
    "    return init_fn\n",
    "\n",
    "\n",
    "def normal(\n",
    "    mean: float = 0.,\n",
    "    std: float = 1.,\n",
    ") -> InitializerFn:\n",
    "    \"\"\" Initialize with a normal distribution.\n",
    "\n",
    "    Args:\n",
    "        mean: The mean of the normal distribution.\n",
    "        std: The standard deviation of the normal distribution.\n",
    "    \"\"\"\n",
    "    def init_fn(key, shape, dtype=jnp.float32):\n",
    "        _mean = lax.convert_element_type(mean, dtype)\n",
    "        _std = lax.convert_element_type(std, dtype)\n",
    "        return _mean + _std * jax.random.normal(key, shape, dtype)\n",
    "\n",
    "    return init_fn\n",
    "\n",
    "\n",
    "def truncated_normal(\n",
    "    mean: float = 0.,\n",
    "    std: float = 1.,\n",
    ") -> InitializerFn:\n",
    "    \"\"\" Initialize with a truncated normal distribution.\n",
    "\n",
    "    Args:\n",
    "        mean: The mean of the truncated normal distribution.\n",
    "        std: The standard deviation of the truncated normal distribution.\n",
    "    \"\"\"\n",
    "    def init_fn(key, shape, dtype=jnp.float32):\n",
    "        _mean = lax.convert_element_type(mean, dtype)\n",
    "        _std = lax.convert_element_type(std, dtype)\n",
    "        return _mean + _std * jax.random.truncated_normal(\n",
    "            key, -2., 2., shape, dtype)\n",
    "\n",
    "    return init_fn\n",
    "\n",
    "\n",
    "# Modules\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def quax_wrapper(layer_fn):\n",
    "    \"\"\" Create a module from a quax layer. \"\"\"\n",
    "    def module(*args, **kwargs):\n",
    "        init_fn, apply_fn = layer_fn(*args, **kwargs)\n",
    "\n",
    "        def _apply_fn(params, state, key, inputs, **kwargs):\n",
    "            outputs = apply_fn(params, inputs, **kwargs)\n",
    "            return outputs, state\n",
    "\n",
    "        def _init_fn(key, inputs_shape):\n",
    "            shape, params = init_fn(key, inputs_shape)\n",
    "            state = None\n",
    "            return params, state, shape\n",
    "\n",
    "        return ModuleFn(_apply_fn, init=_init_fn)\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def haiku_wrapper(layer_fn):\n",
    "    \"\"\" Create a module from a Haiku layer. \"\"\"\n",
    "    def module(*args, **kwargs):\n",
    "        import haiku as hk\n",
    "        layer = hk.transform_with_state(layer_fn(*args, **kwargs))\n",
    "\n",
    "        def _apply_fn(params, state, key, inputs, **kwargs):\n",
    "            outputs, state = layer.apply(params, state, key, inputs, **kwargs)\n",
    "            return outputs, state\n",
    "\n",
    "        def _init_fn(key, inputs_shape):\n",
    "            params, state = layer.init(key, inputs_shape)\n",
    "            outputs, _ = layer.apply(params, state, key, inputs_shape,\n",
    "                                     **kwargs)\n",
    "            shape = outputs.shape\n",
    "            return params, state, shape\n",
    "\n",
    "        return ModuleFn(_apply_fn, init=_init_fn)\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def elementwise(elementwise_fn: Callable[[Array], Array], ) -> ModuleFn:\n",
    "    \"\"\" Create an elementwise layer from a JAX function. \n",
    "\n",
    "        Args:\n",
    "            elementwise_fn: The JAX function to apply to each element.\n",
    "    \"\"\"\n",
    "    return ModuleFn(apply=elementwise_fn)\n",
    "\n",
    "\n",
    "def linear(\n",
    "    n_features: int,\n",
    "    with_bias: bool = True,\n",
    "    w_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create a linear layer.\n",
    "\n",
    "    Args:\n",
    "        n_features: The number of features in the output.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        w_init: The initializer for the weights.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        outputs = jnp.dot(inputs, params['w'])\n",
    "\n",
    "        if with_bias:\n",
    "            outputs += params['b']\n",
    "        return outputs, None\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        params, state = {}, None\n",
    "        key, w_key, b_key = jax.random.split(key, 3)\n",
    "        w_init_ = w_init or truncated_normal(std=1. / inputs_shape[-1])\n",
    "        w_shape = (inputs_shape[-1], n_features)\n",
    "        params['w'] = w_init_(w_key, w_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        shape = inputs_shape[:-1] + (n_features, )\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)\n",
    "\n",
    "\n",
    "def layer_norm(\n",
    "    with_scale: bool = True,\n",
    "    with_bias: bool = True,\n",
    "    s_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create a normalization layer. \n",
    "\n",
    "    Args:\n",
    "        with_scale: Whether to use a scale parameter.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        s_init: The initializer for the scale.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def init_fn(key, inputs_shape):\n",
    "        params = {}\n",
    "        state = None\n",
    "        s_key, b_key = jax.random.split(key)\n",
    "        n_features = inputs_shape[-1]\n",
    "        if with_scale:\n",
    "            s_init_ = s_init or ones()\n",
    "            s_shape = (n_features, )\n",
    "            params['s'] = s_init_(s_key, s_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        return params, state, inputs_shape\n",
    "\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        mean = jnp.mean(inputs, axis=-1, keepdims=True)\n",
    "        var = jnp.var(inputs, axis=-1, keepdims=True) + 1e-5\n",
    "        outputs = params['s'] * (inputs - mean) / jnp.sqrt(var) + params['b']\n",
    "        return outputs, state\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)\n",
    "\n",
    "\n",
    "def sequential(*modules: List[ModuleFn], ) -> ModuleFn:\n",
    "    \"\"\" Create a sequential module from a list of modules.\n",
    "\n",
    "    Args:\n",
    "        modules: A list of modules.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        if key is not None:\n",
    "            key = jax.random.split(key, len(modules))\n",
    "        else:\n",
    "            key = len(modules) * [None]\n",
    "        new_state = dict(\n",
    "            ('layer_{}'.format(idx), None) for idx in range(len(modules)))\n",
    "        if state is None:\n",
    "            state = new_state\n",
    "        for idx, module in enumerate(modules):\n",
    "            if module.init is not None:\n",
    "                outputs, new_module_state = module.apply(\n",
    "                    params['layer_{}'.format(idx)],\n",
    "                    state['layer_{}'.format(idx)],\n",
    "                    key[idx],\n",
    "                    outputs,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                new_state['layer_{}'.format(idx)] = new_module_state\n",
    "            else:\n",
    "                outputs = module.apply(outputs)\n",
    "\n",
    "        state = new_state\n",
    "        return outputs, state\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        params = dict(\n",
    "            ('layer_{}'.format(idx), None) for idx in range(len(modules)))\n",
    "        state = dict(\n",
    "            ('layer_{}'.format(idx), None) for idx in range(len(modules)))\n",
    "        key = jax.random.split(key, len(modules))\n",
    "        shape = inputs_shape\n",
    "        for idx, module in enumerate(modules):\n",
    "            if module.init is not None:\n",
    "                module_params, module_state, shape = module.init(\n",
    "                    key[idx], shape)\n",
    "                params['layer_{}'.format(idx)] = module_params\n",
    "                state['layer_{}'.format(idx)] = module_state\n",
    "            else:\n",
    "                shape = module.apply(jnp.zeros(shape)).shape\n",
    "\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)\n",
    "\n",
    "\n",
    "def orthogonalize_weights(weights):\n",
    "    \"\"\"Take the current weight matrices for each layer, apply SVD decomposition on each one, \n",
    "    then transform the singular values, and finally recompose to make the weight matrix orthogonal.\n",
    "    U,s,V = SVD(W). then all singular values must be ~1. \n",
    "    Output : update the self.weights matrices. \n",
    "    Reference : Orthogonal Deep Neural Networks, K.Juia et al. 2019\"\"\"\n",
    "    epsilon = 0.5\n",
    "    U, s, V = jnp.linalg.svd(weights, full_matrices=False)\n",
    "    s = jnp.clip(s, 1/(1+epsilon), 1+epsilon)\n",
    "    # reform with the new singular values\n",
    "    weights = jnp.dot(U, jnp.dot(jnp.diag(s), V))\n",
    "    return weights\n",
    "\n",
    "\n",
    "def orthogonalize_params(params):\n",
    "    \"\"\"Take a dictionary of params and orthogonalize the weights\n",
    "    \"\"\"\n",
    "    for k1 in params.keys():\n",
    "        if params[k1] != None:\n",
    "            for k2 in params[k1].keys():\n",
    "                if k2.split('/')[-1] == 'w':\n",
    "                    params[k1][k2] = orthogonalize_weights(params[k1][k2])\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def _make_orthogonal_fn(rbs_idxs, size):\n",
    "    num_thetas = sum(map(len, rbs_idxs))\n",
    "    rbs_idxs = [list(map(list, rbs_idx)) for rbs_idx in rbs_idxs]\n",
    "    len_idxs = np.cumsum([0] + list(map(len, rbs_idxs)))\n",
    "\n",
    "    def _get_rbs_unitary(theta):\n",
    "        \"\"\" Returns the unitary matrix for a single RBS gate. \"\"\"\n",
    "        cos_theta, sin_theta = jnp.cos(theta), jnp.sin(theta)\n",
    "        unitary = jnp.array([\n",
    "            [cos_theta, sin_theta],\n",
    "            [-sin_theta, cos_theta],\n",
    "        ])\n",
    "        unitary = unitary.transpose(*[*range(2, unitary.ndim), 0, 1])\n",
    "        return unitary\n",
    "\n",
    "    def _get_rbs_unitary_grad(theta):\n",
    "        \"\"\" Returns the unitary matrix for a single RBS gate. \"\"\"\n",
    "        cos_theta, sin_theta = jnp.cos(theta), jnp.sin(theta)\n",
    "        unitary = jnp.array([\n",
    "            [-sin_theta, cos_theta],\n",
    "            [-cos_theta, -sin_theta],\n",
    "        ])\n",
    "        unitary = unitary.transpose(*[*range(2, unitary.ndim), 0, 1])\n",
    "        return unitary\n",
    "\n",
    "    @jax.custom_jvp\n",
    "    def _get_parallel_rbs_unitary(thetas):\n",
    "        \"\"\" Returns the unitary matrix for parallel RBS gates. \"\"\"\n",
    "        unitaries = []\n",
    "        for i, idxs in enumerate(rbs_idxs):\n",
    "            idxs = sum(idxs, [])\n",
    "            sub_thetas = thetas[len_idxs[i]:len_idxs[i + 1]]\n",
    "            rbs_blocks = _get_rbs_unitary(sub_thetas)\n",
    "            eye_block = jnp.eye(size - len(idxs), dtype=thetas.dtype)\n",
    "            permutation = idxs + [i for i in range(size) if i not in idxs]\n",
    "            permutation = np.argsort(permutation)\n",
    "            unitary = jax.scipy.linalg.block_diag(*rbs_blocks, eye_block)\n",
    "            unitary = unitary[permutation][:, permutation]\n",
    "            unitaries.append(unitary)\n",
    "        unitaries = jnp.stack(unitaries)\n",
    "        return unitaries\n",
    "\n",
    "    @_get_parallel_rbs_unitary.defjvp\n",
    "    def get_parallel_rbs_unitary_jvp(primals, tangents):\n",
    "        thetas, = primals\n",
    "        thetas_dot, = tangents\n",
    "        unitaries = []\n",
    "        unitaries_dot = []\n",
    "        for i, idxs in enumerate(rbs_idxs):\n",
    "            idxs = sum(idxs, [])\n",
    "            sub_thetas = thetas[len_idxs[i]:len_idxs[i + 1]]\n",
    "            sub_thetas_dot = thetas_dot[len_idxs[i]:len_idxs[i + 1]]\n",
    "            rbs_blocks = _get_rbs_unitary(sub_thetas)\n",
    "            rbs_blocks_grad = _get_rbs_unitary_grad(sub_thetas)\n",
    "            rbs_blocks_dot = sub_thetas_dot[..., None, None] * rbs_blocks_grad\n",
    "            eye_block = jnp.eye(size - len(idxs), dtype=thetas.dtype)\n",
    "            zero_block = jnp.zeros_like(eye_block)\n",
    "            permutation = idxs + [i for i in range(size) if i not in idxs]\n",
    "            permutation = np.argsort(permutation)\n",
    "            unitary = jax.scipy.linalg.block_diag(*rbs_blocks, eye_block)\n",
    "            unitary_dot = jax.scipy.linalg.block_diag(*rbs_blocks_dot,\n",
    "                                                      zero_block)\n",
    "            unitary = unitary[permutation][:, permutation]\n",
    "            unitary_dot = unitary_dot[permutation][:, permutation]\n",
    "            unitaries.append(unitary)\n",
    "            unitaries_dot.append(unitary_dot)\n",
    "        primal_out = jnp.stack(unitaries)\n",
    "        tangent_out = jnp.stack(unitaries_dot)\n",
    "        return primal_out, tangent_out\n",
    "\n",
    "    def orthogonal_fn(thetas, precision=None):\n",
    "        \"\"\" Returns the unitary matrix for a sequence of parallel RBS gates. \"\"\"\n",
    "        assert thetas.shape[0] == num_thetas, \"Wrong number of thetas.\"\n",
    "        unitaries = _get_parallel_rbs_unitary(thetas)\n",
    "        unitary = jnp.linalg.multi_dot(unitaries[::-1], precision=precision)\n",
    "        return unitary\n",
    "\n",
    "    return orthogonal_fn\n",
    "\n",
    "def make_general_orthogonal_fn(rbs_idxs, size):\n",
    "    num_thetas = sum(map(len, rbs_idxs))\n",
    "    rbs_idxs = [list(map(list, rbs_idx)) for rbs_idx in rbs_idxs]\n",
    "    len_idxs = np.cumsum([0] + list(map(len, rbs_idxs)))\n",
    "\n",
    "    def _get_rbs_unitary(theta):\n",
    "        \"\"\" Returns the unitary matrix for a single RBS gate. \"\"\"\n",
    "        cos_t, sin_t = jnp.cos(theta), jnp.sin(theta)\n",
    "        zeros = jnp.zeros_like(cos_t)\n",
    "        ones = jnp.ones_like(cos_t)\n",
    "        unitary = jnp.array([\n",
    "            [ones, zeros, zeros, zeros],\n",
    "            [zeros, cos_t, -sin_t, zeros],\n",
    "            [zeros, sin_t, cos_t, zeros],\n",
    "            [zeros, zeros, zeros, ones],\n",
    "        ])\n",
    "        unitary = unitary.transpose(*[*range(2, unitary.ndim), 0, 1])\n",
    "        return unitary\n",
    "\n",
    "    def _get_parallel_rbs_unitary(thetas):\n",
    "        \"\"\" Returns the unitary matrix for parallel RBS gates. \"\"\"\n",
    "        unitaries = []\n",
    "        num_qubits = size\n",
    "        map_qubits = [[0, 2**q] for q in range(num_qubits)]\n",
    "        for i, idxs in enumerate(rbs_idxs):\n",
    "            idxs = sum(idxs, [])\n",
    "            sub_thetas = thetas[len_idxs[i]:len_idxs[i + 1]]\n",
    "            rbs_blocks = _get_rbs_unitary(sub_thetas)\n",
    "            eye_block = jnp.eye(2**(size - len(idxs)) , dtype=thetas.dtype)\n",
    "            unitary =  tensordot_unitary([*rbs_blocks, eye_block])\n",
    "            unitary_qubits = idxs + [\n",
    "            q for q in range(num_qubits) if q not in idxs\n",
    "            ]\n",
    "            permutation = np.argsort([\n",
    "            sum(binary)\n",
    "            for binary in itertools.product(*(map_qubits[q]\n",
    "                                              for q in unitary_qubits)) \n",
    "            ])\n",
    "            unitary = unitary[permutation][:, permutation]\n",
    "            unitaries.append(unitary)\n",
    "        unitaries = jnp.stack(unitaries)\n",
    "        \n",
    "        return unitaries\n",
    "\n",
    "\n",
    "    def orthogonal_fn(thetas, precision=None):\n",
    "        \"\"\" Returns the unitary matrix for a sequence of parallel RBS gates. \"\"\"\n",
    "        assert thetas.shape[0] == num_thetas, \"Wrong number of thetas.\"\n",
    "        unitaries = _get_parallel_rbs_unitary(thetas)\n",
    "        unitary = jnp.linalg.multi_dot(unitaries[::-1], precision=precision)\n",
    "        return unitary\n",
    "\n",
    "    return orthogonal_fn\n",
    "\n",
    "def _get_pyramid_idxs(num_inputs, num_outputs):\n",
    "    num_max = max(num_inputs, num_outputs)\n",
    "    num_min = min(num_inputs, num_outputs)\n",
    "    if num_max == num_min:\n",
    "        num_min -= 1\n",
    "    end_idxs = np.concatenate(\n",
    "        [np.arange(1, num_max - 1), num_max - np.arange(1, num_min + 1)])\n",
    "    start_idxs = np.concatenate([\n",
    "        np.arange(end_idxs.shape[0] + num_min - num_max) % 2,\n",
    "        np.arange(num_max - num_min)\n",
    "    ])\n",
    "    if num_inputs < num_outputs:\n",
    "        start_idxs = start_idxs[::-1]\n",
    "        end_idxs = end_idxs[::-1]\n",
    "    rbs_idxs = [\n",
    "        np.arange(start_idxs[i], end_idxs[i] + 1).reshape(-1, 2)\n",
    "        for i in range(len(start_idxs))\n",
    "    ]\n",
    "    return rbs_idxs\n",
    "\n",
    "\n",
    "def _get_butterfly_idxs(num_inputs, num_outputs):\n",
    "    def _get_butterfly_idxs(n):\n",
    "        if n == 2:\n",
    "            return np.array([[[0, 1]]])\n",
    "        else:\n",
    "            rbs_idxs = _get_butterfly_idxs(n // 2)\n",
    "            first = np.concatenate([rbs_idxs, rbs_idxs + n // 2], 1)\n",
    "            last = np.arange(n).reshape(1, 2, n // 2).transpose(0, 2, 1)\n",
    "            rbs_idxs = np.concatenate([first, last], 0)\n",
    "            return rbs_idxs\n",
    "\n",
    "    circuit_dim = int(2**np.ceil(np.log2(max(num_inputs, num_outputs))))\n",
    "    rbs_idxs = _get_butterfly_idxs(circuit_dim)\n",
    "    if num_inputs < num_outputs:\n",
    "        rbs_idxs = rbs_idxs[::-1]\n",
    "    return rbs_idxs\n",
    "\n",
    "\n",
    "def ortho_linear(\n",
    "    n_features: int,\n",
    "    layout: Union[str, List[List[Tuple[int, int]]]] = 'butterfly',\n",
    "    normalize_inputs: bool = False,\n",
    "    normalize_outputs: bool = True,\n",
    "    normalize_stop_gradient: bool = True,\n",
    "    with_scale: bool = True,\n",
    "    with_bias: bool = True,\n",
    "    t_init: Optional[InitializerFn] = None,\n",
    "    s_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create an orthogonal layer from a layout of RBS gates.\n",
    "\n",
    "    Args:\n",
    "        n_features: The number of features in the output.\n",
    "        layout: The layout of the RBS gates.\n",
    "        normalize_inputs: Whether to normalize the inputs.\n",
    "        normalize_outputs: Whether to normalize the outputs.\n",
    "        normalize_stop_gradient: Whether to stop the gradient of the norm.\n",
    "        with_scale: Whether to use a scale parameter.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        t_init: The initializer for the angles.\n",
    "        s_init: The initializer for the scale.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = int(2**np.ceil(\n",
    "                np.log2(max(inputs.shape[-1], n_features))))\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            make_unitary = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = max(inputs.shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "            circuit_dim = max(\n",
    "                [max(idxs) for moment in layout for idxs in moment])\n",
    "        make_unitary = _make_orthogonal_fn(rbs_idxs[::-1], circuit_dim)\n",
    "        if normalize_inputs:\n",
    "            norm = jnp.linalg.norm(inputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            inputs /= norm\n",
    "        if inputs.shape[-1] < circuit_dim:\n",
    "            zeros = jnp.zeros(\n",
    "                (*inputs.shape[:-1], circuit_dim - inputs.shape[-1]), )\n",
    "            inputs = jnp.concatenate([zeros, inputs], axis=-1)\n",
    "        unitary = make_unitary(params['t'][::-1])\n",
    "        outputs = jnp.dot(inputs, unitary.T)[..., -n_features:]\n",
    "        if normalize_outputs:\n",
    "            norm = jnp.linalg.norm(outputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            outputs /= norm\n",
    "        if with_scale:\n",
    "            outputs *= params['s']\n",
    "        if with_bias:\n",
    "            outputs += params['b']\n",
    "        return outputs, None\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs_shape[-1], n_features)\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs_shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "        n_angles = sum(map(len, rbs_idxs))\n",
    "        params, state = {}, None\n",
    "        key, t_key, b_key, s_key = jax.random.split(key, 4)\n",
    "        t_init_ = t_init or uniform(-np.pi, np.pi)\n",
    "        t_shape = (n_angles, )\n",
    "        params['t'] = t_init_(t_key, t_shape)\n",
    "        if with_scale:\n",
    "            s_init_ = s_init or ones()\n",
    "            s_shape = (n_features, )\n",
    "            params['s'] = s_init_(s_key, s_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        shape = inputs_shape[:-1] + (n_features, )\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)\n",
    "\n",
    "\n",
    "\n",
    "def ortho_linear_noisy(\n",
    "    n_features: int,\n",
    "    noise_scale: float = 0.01,\n",
    "    layout: Union[str, List[List[Tuple[int, int]]]] = 'butterfly',\n",
    "    normalize_inputs: bool = True,\n",
    "    normalize_outputs: bool = True,\n",
    "    normalize_stop_gradient: bool = True,\n",
    "    with_scale: bool = True,\n",
    "    with_bias: bool = True,\n",
    "    t_init: Optional[InitializerFn] = None,\n",
    "    s_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create an orthogonal layer from a layout of RBS gates.\n",
    "    Args:\n",
    "        n_features: The number of features in the output.\n",
    "        layout: The layout of the RBS gates.\n",
    "        normalize_inputs: Whether to normalize the inputs.\n",
    "        normalize_outputs: Whether to normalize the outputs.\n",
    "        normalize_stop_gradient: Whether to stop the gradient of the norm.\n",
    "        with_scale: Whether to use a scale parameter.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        t_init: The initializer for the angles.\n",
    "        s_init: The initializer for the scale.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = int(2**np.ceil(\n",
    "                np.log2(max(inputs.shape[-1], n_features))))\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            make_unitary = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = max(inputs.shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "            circuit_dim = max(\n",
    "                [max(idxs) for moment in layout for idxs in moment])\n",
    "        make_unitary = _make_orthogonal_fn(rbs_idxs[::-1], circuit_dim)\n",
    "        if normalize_inputs:\n",
    "            norm = jnp.linalg.norm(inputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            inputs /= norm\n",
    "        if inputs.shape[-1] < circuit_dim:\n",
    "            zeros = jnp.zeros(\n",
    "                (*inputs.shape[:-1], circuit_dim - inputs.shape[-1]), )\n",
    "            inputs = jnp.concatenate([zeros, inputs], axis=-1)\n",
    "        unitary = make_unitary(params['t'][::-1])\n",
    "        outputs = jnp.dot(inputs, unitary.T)\n",
    "        if normalize_outputs:\n",
    "            norm = jnp.linalg.norm(outputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            outputs /= norm\n",
    "        outputs = jnp.einsum('...i,...i->...i',outputs,outputs)\n",
    "        outputs = outputs[..., -n_features:]\n",
    "        if with_scale:\n",
    "            outputs *= params['s']\n",
    "        if with_bias:\n",
    "            outputs += params['b']\n",
    "        key, _ = jax.random.split(key)\n",
    "        outputs += noise_scale*jax.random.normal(key, outputs.shape)\n",
    "        return outputs, state\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs_shape[-1], n_features)\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs_shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "        n_angles = sum(map(len, rbs_idxs))\n",
    "        params, state = {}, None\n",
    "        key, t_key, b_key, s_key = jax.random.split(key, 4)\n",
    "        t_init_ = t_init or uniform(-np.pi, np.pi)\n",
    "        t_shape = (n_angles, )\n",
    "        params['t'] = t_init_(t_key, t_shape)\n",
    "        if with_scale:\n",
    "            s_init_ = s_init or ones()\n",
    "            s_shape = (n_features, )\n",
    "            params['s'] = s_init_(s_key, s_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        shape = inputs_shape[:-1] + (n_features, )\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5IfE-WEzjqL"
   },
   "source": [
    "# Main\n",
    "\n",
    "## Circuit constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3g2FYe7qX20R"
   },
   "outputs": [],
   "source": [
    "# Global counter\n",
    "\n",
    "global_number_of_circuits_executed = 0\n",
    "\n",
    "# Global object keeping track of result\n",
    "# Used for pickling\n",
    "# Populated initially in DeepHedgingBenchmark().__test_model\n",
    "# and with run results in run_circuit\n",
    "\n",
    "global_hardware_run_results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1QEKbaJDOjXK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from qnn import _get_butterfly_idxs, _get_pyramid_idxs, _make_orthogonal_fn\n",
    "# fix for older versions of Qiskit\n",
    "if qiskit.__version__ <= '0.37.1':\n",
    "    import qiskit.providers.aer.noise as noise\n",
    "else:\n",
    "    import qiskit_aer.noise as noise\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def RBS_gate(theta, bla):\n",
    "\n",
    "    def operator_function(parameters):\n",
    "            theta = parameters['theta']\n",
    "            c = np.cos(theta)\n",
    "            s = np.sin(theta)\n",
    "            return np.array([[1.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, c, s, 0.0],\n",
    "                             [0.0, -s, c, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 1.0]], dtype=np.complex128)\n",
    "\n",
    "    return quasar.Gate(\n",
    "        nqubit=2,\n",
    "        operator_function=operator_function,\n",
    "        parameters=collections.OrderedDict([('theta', theta)]),\n",
    "        name='RBS',\n",
    "        ascii_symbols=['B', 'S'])\n",
    "\n",
    "\n",
    "\n",
    "def prepare_circuit(input, params, loader_layout='parallel', layer_layout='butterfly'):\n",
    "    def _get_layer_circuit():\n",
    "      _params = np.array(params).astype('float')\n",
    "      if layer_layout == 'butterfly':\n",
    "        rbs_idxs = _get_butterfly_idxs(num_qubits, num_qubits)\n",
    "      elif layer_layout == 'pyramid':\n",
    "        rbs_idxs = _get_pyramid_idxs(num_qubits, num_qubits)\n",
    "      circuit_layer = quasar.Circuit()\n",
    "      idx_angle = 0\n",
    "      for gates_per_timestep in rbs_idxs[::-1]:\n",
    "        for gate in gates_per_timestep:\n",
    "          circuit_layer.add_gate(quasar.Gate.RBS(theta=-_params[::-1][idx_angle]), tuple(gate))\n",
    "          idx_angle+=1\n",
    "      return circuit_layer\n",
    "    \n",
    "    num_qubits = len(input)\n",
    "    loader_circuit = loader(np.array(input),mode=loader_layout,initial=True,controlled=False)\n",
    "    layer_circuit = _get_layer_circuit()\n",
    "    circuit = quasar.Circuit.join_in_time([loader_circuit, layer_circuit])\n",
    "    # Translate from qcware-quasar to qiskit\n",
    "    qiskit_circuit = translate(circuit)\n",
    "    \n",
    "    # qiskit_circuit.save_statevector()    \n",
    "\n",
    "    qiskit_circuit = qiskit.transpile(qiskit_circuit, optimization_level=3)\n",
    "    c = qiskit.ClassicalRegister(num_qubits)\n",
    "    qiskit_circuit.add_register(c)\n",
    "    qiskit_circuit.barrier()\n",
    "    qiskit_circuit.measure(qubit=range(num_qubits),cbit=c)\n",
    "    return qiskit_circuit\n",
    "\n",
    "def counter_to_dict(c):\n",
    "    \"\"\"Converts counter returned by pytket get_counts function\n",
    "    to dictionary returned by qiskit\n",
    "    canonical use:\n",
    "    >>> result = backend.get_result(handle)\n",
    "    >>> counts = result.get_counts(basis=BasisOrder.dlo)\n",
    "    >>> counts_qiskit = counter_to_dict(counts)\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for k, v in c.items():\n",
    "        d[''.join(str(x) for x in k)] = int(v)\n",
    "    return d\n",
    "\n",
    "def run_circuit(circs, backend_name = 'quantinuum_H1-2E'):\n",
    "    \"\"\"\n",
    "    backend name accepted \n",
    "    \"\"\"\n",
    "    global global_number_of_circuits_executed\n",
    "    global global_hardware_run_results_dict\n",
    "    input_size = 8\n",
    "    results = np.zeros((len(circs), input_size))\n",
    "    \n",
    "    #TODO\n",
    "    # if 'qiskit' in backend:\n",
    "    #elif 'quantinuum' in backend:\n",
    "    \n",
    "    global_number_of_circuits_executed += len(circs)\n",
    "    num_measurements = 1000\n",
    "    \n",
    "    if \"qiskit\" in backend_name:\n",
    "        backend = qiskit.Aer.get_backend('qasm_simulator')\n",
    "        if backend_name == 'qiskit_noiseless':\n",
    "            measurement = qiskit.execute(circs, backend, shots=num_measurements)\n",
    "        elif backend_name == 'qiskit_noisy': \n",
    "            # Error probabilities\n",
    "            prob_1 = 0.001  # 1-qubit gate\n",
    "            prob_2 = 0.01   # 2-qubit gate\n",
    "            # Dylan's tunes error probabilities\n",
    "            # prob_1 = 0  # 1-qubit gate\n",
    "            # prob_2 = 3.5e-3   # 2-qubit gate\n",
    "\n",
    "            # Depolarizing quantum errors\n",
    "            error_1 = noise.depolarizing_error(prob_1, 1)\n",
    "            error_2 = noise.depolarizing_error(prob_2, 2)\n",
    "\n",
    "            # Add errors to noise model\n",
    "            noise_model = noise.NoiseModel()\n",
    "            noise_model.add_all_qubit_quantum_error(error_1, ['h', 'x', 'ry'])\n",
    "            noise_model.add_all_qubit_quantum_error(error_2, ['cz'])\n",
    "\n",
    "            # Get basis gates from noise model\n",
    "            basis_gates = noise_model.basis_gates\n",
    "            measurement = qiskit.execute(circs, backend,basis_gates=basis_gates, noise_mode=noise_model, shots=num_measurements)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected backend name {backend_name}\")\n",
    "        all_counts = measurement.result().get_counts()\n",
    "    elif \"quantinuum\" in backend_name:\n",
    "        # From docs: \"Batches cannot exceed the maximum limit of 500 H-System Quantum Credits (HQCs) total\"\n",
    "        # Therefore batching is more or less useless on quantinuum\n",
    "        from pytket.extensions.qiskit import qiskit_to_tk\n",
    "        from pytket.circuit import BasisOrder\n",
    "        from pytket.extensions.quantinuum import QuantinuumBackend\n",
    "     \n",
    "        if global_hardware_run_results_dict['model_type'] != 'simple':\n",
    "            raise NotImplementedError(f\"Model {global_hardware_run_results_dict['model_type']} not supported yet, only simple model is supported.\")\n",
    "    \n",
    "        outpath_stem = f\"1031_{global_hardware_run_results_dict['model_type']}_{backend_name}_{global_hardware_run_results_dict['layer_type']}_{global_hardware_run_results_dict['epsilon']}\"\n",
    "        \n",
    "        outpath_result_final = f\"data/{outpath_stem}.json\"\n",
    "        outpath_handles = f\"data/handles_{outpath_stem}.pickle\"\n",
    "        if Path(outpath_result_final).exists():\n",
    "            # if precomputed results already present on disk, simply load\n",
    "            print(f\"Using precomputed counts from {outpath_result_final}\")\n",
    "            all_counts = json.load(open(outpath_result_final, \"r\"))['all_counts']\n",
    "        else:\n",
    "            if backend_name == \"quantinuum_H1-2E\":\n",
    "                backend = QuantinuumBackend(device_name=\"H1-2E\")\n",
    "            elif backend_name == \"quantinuum_H1-2\":\n",
    "                backend = QuantinuumBackend(device_name=\"H1-2\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown Quantinuum backend: {backend_name}\")\n",
    "            if Path(outpath_handles).exists():\n",
    "                # if circuits already submitted, simply load from disk\n",
    "                print(f\"Using pickled handles from {outpath_handles}\")\n",
    "                handles = pickle.load(open(outpath_handles, \"rb\"))\n",
    "            else:\n",
    "                # otherwise, submit circuits and pickle handles\n",
    "                circs_tk = [qiskit_to_tk(circ) for circ in circs]\n",
    "                for idx, circ in enumerate(circs_tk):\n",
    "                    circ.name = f'{outpath_stem}_{idx+1}_of_{len(circs)}'\n",
    "                compiled_circuits = backend.get_compiled_circuits(circs_tk, optimisation_level=2)\n",
    "                handles = backend.process_circuits(compiled_circuits, n_shots=num_measurements)\n",
    "                pickle.dump(handles, open(outpath_handles, \"wb\"))\n",
    "                print(f\"Dumped handles to {outpath_handles}\")\n",
    "            # retrieve results from handles\n",
    "            result_list = []\n",
    "            \n",
    "            with tqdm(total=len(handles), desc='#jobs finished') as pbar:\n",
    "                for handle in handles:\n",
    "                    while True:\n",
    "                        status = backend.circuit_status(handle).status\n",
    "                        if status.name == 'COMPLETED':\n",
    "                            result = backend.get_result(handle)\n",
    "                            result_list.append(copy.deepcopy(result))\n",
    "                            pbar.update(1)\n",
    "                            break\n",
    "                        else:\n",
    "                            assert status.name in ['QUEUED', 'RUNNING'] \n",
    "                        time.sleep(1)\n",
    "            global_hardware_run_results_dict['result_list'] = [x.to_dict() for x in result_list]\n",
    "            # convert from tket counts format to qiskit\n",
    "            all_counts = [\n",
    "                counter_to_dict(\n",
    "                    result.get_counts(basis=BasisOrder.dlo)\n",
    "                ) for result in result_list\n",
    "            ]\n",
    "            global_hardware_run_results_dict['all_counts'] = all_counts\n",
    "            # dump result on disk\n",
    "            json.dump(global_hardware_run_results_dict, open(outpath_result_final, \"w\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected backend name {backend_name}\")\n",
    "        \n",
    "    # Post processing\n",
    "    # Discard bitstrings that do not correspond to unary encoding (not Hamming weight 1)\n",
    "    # We build a dictionary with all unary bitstrings and only add counts corresponding to unary bitstrings\n",
    "    # Note: f\"{2**i:0{input_size}b}\" converts 2**i to its binary string representation.\n",
    "    for j in range(len(circs)):\n",
    "        measurementRes = all_counts[j]\n",
    "        num_postselected = 0\n",
    "        filtered_counts = {f\"{2**i:0{input_size}b}\":0 for i in range(input_size)}\n",
    "        for bitstring, count in measurementRes.items():\n",
    "            if sum([int(x) for x in bitstring]) != 1:\n",
    "                continue\n",
    "            filtered_counts[bitstring] += count\n",
    "            num_postselected+= count\n",
    "        results[j] = [filtered_counts[k]/num_postselected for k in sorted(filtered_counts)][::-1]    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__w2M8wQskTn"
   },
   "source": [
    "## Definition of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TqFtMKbVhkR9"
   },
   "outputs": [],
   "source": [
    "\n",
    "def ortho_linear_hardware(\n",
    "    n_features: int,\n",
    "    layout: Union[str, List[List[Tuple[int, int]]]] = 'butterfly',\n",
    "    normalize_inputs: bool = True,\n",
    "    normalize_outputs: bool = True,\n",
    "    normalize_stop_gradient: bool = True,\n",
    "    with_scale: bool = True,\n",
    "    with_bias: bool = True,\n",
    "    t_init: Optional[InitializerFn] = None,\n",
    "    s_init: Optional[InitializerFn] = None,\n",
    "    b_init: Optional[InitializerFn] = None,\n",
    ") -> ModuleFn:\n",
    "    \"\"\" Create an orthogonal layer from a layout of RBS gates.\n",
    "    Args:\n",
    "        n_features: The number of features in the output.\n",
    "        layout: The layout of the RBS gates.\n",
    "        normalize_inputs: Whether to normalize the inputs.\n",
    "        normalize_outputs: Whether to normalize the outputs.\n",
    "        normalize_stop_gradient: Whether to stop the gradient of the norm.\n",
    "        with_scale: Whether to use a scale parameter.\n",
    "        with_bias: Whether to include a bias term.\n",
    "        t_init: The initializer for the angles.\n",
    "        s_init: The initializer for the scale.\n",
    "        b_init: The initializer for the bias.\n",
    "    \"\"\"\n",
    "    def apply_fn(params, state, key, inputs, **kwargs):\n",
    "        # Step 1: preprocess the inputs\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = int(2**np.ceil(\n",
    "                np.log2(max(inputs.shape[-1], n_features))))\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            make_unitary = _get_pyramid_idxs(inputs.shape[-1], n_features)\n",
    "            circuit_dim = max(inputs.shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "            circuit_dim = max(\n",
    "                [max(idxs) for moment in layout for idxs in moment])\n",
    "        if normalize_inputs:\n",
    "            norm = jnp.linalg.norm(inputs, axis=-1)[..., None]\n",
    "            if normalize_stop_gradient:\n",
    "                norm = lax.stop_gradient(norm)\n",
    "            inputs /= norm\n",
    "        if inputs.shape[-1] < circuit_dim:\n",
    "            zeros = jnp.zeros(\n",
    "                (*inputs.shape[:-1], circuit_dim - inputs.shape[-1]), )\n",
    "            inputs = jnp.concatenate([zeros, inputs], axis=-1)\n",
    "        # Step 2: generate the circuits\n",
    "        circs = []\n",
    "        out_shape = inputs.shape[:-1]+(n_features,)\n",
    "        for input in inputs.reshape(-1,circuit_dim):\n",
    "            circs.append(prepare_circuit(input,params['t']))\n",
    "        # run circuits and truncate to desired number of outputs\n",
    "        outputs = jnp.array(run_circuit(circs))[..., -n_features:]\n",
    "        \n",
    "        outputs = outputs.reshape(out_shape)\n",
    "        # unitary = make_unitary(params['t'])\n",
    "        # outputs = jnp.dot(inputs, unitary.T)[..., -n_features:]\n",
    "        # outputs = inputs\n",
    "        if with_scale:\n",
    "            outputs *= params['s']\n",
    "        if with_bias:\n",
    "            outputs += params['b']\n",
    "        return outputs, state\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        if layout == 'butterfly':\n",
    "            rbs_idxs = _get_butterfly_idxs(inputs_shape[-1], n_features)\n",
    "        elif layout == 'pyramid':\n",
    "            rbs_idxs = _get_pyramid_idxs(inputs_shape[-1], n_features)\n",
    "        else:\n",
    "            rbs_idxs = layout\n",
    "        n_angles = sum(map(len, rbs_idxs))\n",
    "        params, state = {}, None\n",
    "        key, t_key, b_key, s_key = jax.random.split(key, 4)\n",
    "        t_init_ = t_init or uniform(-np.pi, np.pi)\n",
    "        t_shape = (n_angles, )\n",
    "        params['t'] = t_init_(t_key, t_shape)\n",
    "        if with_scale:\n",
    "            s_init_ = s_init or ones()\n",
    "            s_shape = (n_features, )\n",
    "            params['s'] = s_init_(s_key, s_shape)\n",
    "        if with_bias:\n",
    "            b_init_ = b_init or zeros()\n",
    "            b_shape = (n_features, )\n",
    "            params['b'] = b_init_(b_key, b_shape)\n",
    "        shape = inputs_shape[:-1] + (n_features, )\n",
    "        return params, state, shape\n",
    "\n",
    "    return ModuleFn(apply_fn, init=init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6TcMo24PAKh"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YjoX6Hojg1Ev"
   },
   "outputs": [],
   "source": [
    "from typing import Any, TypeVar\n",
    "import itertools\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import qnn\n",
    "from qnn import ModuleFn, elementwise, linear, sequential, make_general_orthogonal_fn, _get_butterfly_idxs, _get_pyramid_idxs \n",
    "\n",
    "\n",
    "relu = elementwise(jax.nn.relu)\n",
    "gelu = elementwise(jax.nn.gelu)\n",
    "log_softmax = elementwise(jax.nn.log_softmax)\n",
    "sigmoid = elementwise(jax.nn.sigmoid)\n",
    "\n",
    "def scan(f, init, xs, length=None):\n",
    "  if xs is None:\n",
    "    xs = [None] * length\n",
    "  carry = init\n",
    "  ys = []\n",
    "  for x in xs:\n",
    "    carry, y = f(carry, x)\n",
    "    ys.append(y)\n",
    "  return carry, jnp.stack(ys)\n",
    "\n",
    "def recurrent_network(hps, layer_func: ModuleFn = linear, **kwargs) -> ModuleFn:\n",
    "    \"\"\" Create a Recurrent Network.\n",
    "    Args:\n",
    "        n_features: The number of features.\n",
    "        n_layers: The number of layers.\n",
    "        layer_func: The type of layers to use.\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessing = [linear(hps.n_features), sigmoid]\n",
    "    features = hps.n_layers * [layer_func(hps.n_features), relu]\n",
    "    postprocessing = [linear(1), sigmoid]\n",
    "    layers = preprocessing + features + postprocessing\n",
    "    net = sequential(*layers)\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        params = net.init(\n",
    "            key, (inputs_shape[0], inputs_shape[1], 2*inputs_shape[2]))[0]\n",
    "        return params, None, inputs_shape\n",
    "\n",
    "    def apply_fn(params, state, key, inputs):\n",
    "        def cell_fn(prev_outputs, inputs):\n",
    "            inp = inputs[None, ...]\n",
    "            inp = jnp.concatenate([prev_outputs, inp], axis=-1)\n",
    "            delta = net.apply(params, None, key, inp)[0]\n",
    "            # print(f'inputs shape = {inp.shape} deltas shape = {delta.shape}')\n",
    "            return delta, delta\n",
    "\n",
    "        prev_state = jnp.zeros((1, inputs.shape[0], inputs.shape[-1]))\n",
    "        inputs = inputs.transpose(1, 0, 2)\n",
    "        _, outputs = scan(cell_fn, prev_state, inputs)\n",
    "        outputs = jnp.squeeze(outputs, 1)\n",
    "        outputs = outputs.transpose(1, 0, 2)\n",
    "        return outputs, state\n",
    "    return qnn.ModuleFn(apply_fn, init_fn)\n",
    "\n",
    "\n",
    "def lstm_cell(hps,  layer_func: ModuleFn = linear, **kwargs) -> ModuleFn:\n",
    "    \"\"\" Create an LSTM Cell.\n",
    "    Args:\n",
    "        n_features: The number of features.\n",
    "        layer_func: The type of layers to use.\n",
    "    \"\"\"\n",
    "\n",
    "    _linear = layer_func(n_features=int(hps.n_features/2), with_bias=True)\n",
    "\n",
    "    def init_fn(key, inputs_shape):\n",
    "        keys = jax.random.split(key, num=4)\n",
    "        params = {}\n",
    "        layer_idx = ['i', 'g', 'f', 'o']\n",
    "        _shape = (inputs_shape[0], inputs_shape[1], 2*inputs_shape[2])\n",
    "        _init_params = {}\n",
    "        for i, id in enumerate(layer_idx):\n",
    "            _init_params[id] = _linear.init(keys[i],  _shape)[0]\n",
    "            params.update(qnn.add_scope_to_params(id, _init_params[id]))\n",
    "        return params, None, inputs_shape\n",
    "\n",
    "    def apply_fn(params, state, key, inputs):\n",
    "        def cell_fn(prev_state, inputs):\n",
    "            prev_hidden, prev_cell = prev_state\n",
    "            x_and_h = jnp.concatenate([inputs, prev_hidden], axis=-1)\n",
    "            layer_idx = ['i', 'g', 'f', 'o']\n",
    "            _apply_params = {}\n",
    "            for i, id in enumerate(layer_idx):\n",
    "                _apply_params[id] = qnn.get_params_by_scope(id, params)\n",
    "            i = _linear.apply(_apply_params['i'], None, key, x_and_h)[0]\n",
    "            g = _linear.apply(_apply_params['g'], None, key, x_and_h)[0]\n",
    "            f = _linear.apply(_apply_params['f'], None, key, x_and_h)[0]\n",
    "            o = _linear.apply(_apply_params['o'], None, key, x_and_h)[0]\n",
    "            # i = input, g = cell_gate, f = forget_gate, o = output_gate\n",
    "            f = jax.nn.sigmoid(f + 1)\n",
    "            c = f * prev_cell + jax.nn.sigmoid(i) * jnp.tanh(g)\n",
    "            h = jax.nn.sigmoid(o) * jnp.tanh(c)\n",
    "            return jnp.stack([h, c], axis=0), h\n",
    "\n",
    "        prev_state = jnp.zeros((2, inputs.shape[0], inputs.shape[-1]))\n",
    "        inputs = inputs.transpose(1, 0, 2)\n",
    "        _, outputs = scan(cell_fn, prev_state, inputs)\n",
    "        outputs = outputs.transpose(1, 0, 2)\n",
    "        return outputs, state\n",
    "    return qnn.ModuleFn(apply_fn, init_fn)\n",
    "\n",
    "\n",
    "def lstm_network(hps, layer_func: ModuleFn = linear, **kwargs) -> ModuleFn:\n",
    "    \"\"\" Create an LSTM Network.\n",
    "    Args:\n",
    "        n_features: The number of features.\n",
    "        layer_func: The type of layers to use.\n",
    "    \"\"\"\n",
    "    preprocessing = [linear( int(hps.n_features/2) ), sigmoid]\n",
    "    features = [lstm_cell(hps=hps, layer_func=layer_func)]\n",
    "    postprocessing = [linear(1), sigmoid]\n",
    "    layers = preprocessing + features + postprocessing\n",
    "    net = sequential(*layers)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOnoeMTmz7ZO",
    "outputId": "f9b9e67a-764b-4f36-a374-5ced216983a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "100%|██████████| 5/5 [00:00<00:00, 1907.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# from models import simple_network, attention_network\n",
    "from qnn import linear\n",
    "from train import build_train_fn\n",
    "from qnn import ortho_linear, ortho_linear_noisy\n",
    "from models import simple_network, attention_network\n",
    "from loss_metrics import entropy_loss\n",
    "from data import gen_paths\n",
    "from utils import train_test_split, get_batches, HyperParams\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "from functools import partial \n",
    "from utils import HyperParams\n",
    "seed = 100\n",
    "key = jax.random.PRNGKey(seed)\n",
    "hps = HyperParams(S0=100,\n",
    "                  n_steps=5,\n",
    "                  n_paths=10000,\n",
    "                  discrete_path=False,\n",
    "                  strike_price=100,\n",
    "                  epsilon=0.0,\n",
    "                  sigma=0.2,\n",
    "                  risk_free=0,\n",
    "                  dividend=0,\n",
    "                  model_type='simple',\n",
    "                  layer_type='noisy_ortho',\n",
    "                  n_features=8,\n",
    "                  n_layers=1,\n",
    "                  loss_param=1.0,\n",
    "                  batch_size=5,\n",
    "                  test_size=0.2,\n",
    "                  optimizer='adam',\n",
    "                  learning_rate=1E-3,\n",
    "                  num_epochs=100\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "# Data\n",
    "S = gen_paths(hps)\n",
    "[S_train, S_test] = train_test_split([S], test_size=0.2)\n",
    "_, test_batches = get_batches(jnp.array(S_test[0]), batch_size=hps.batch_size)\n",
    "test_batch = test_batches[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fVm_ZjYhHCQu"
   },
   "outputs": [],
   "source": [
    "from utils import load_params\n",
    "class DeepHedgingBenchmark():\n",
    "  \"\"\"\n",
    "  Runs the benchmark with different models / layers\n",
    "  Input: test_batch above\n",
    "  test_batch has 8 datapoints\n",
    "  \"\"\"\n",
    "  def __init__(self, key, eps,  layers, models):\n",
    "      self.__key = key\n",
    "      self.__models = models\n",
    "      self.__layers = layers\n",
    "      self.__eps = eps\n",
    "      self.test_info = {layer:{str(eps):{} for eps in self.__eps} for layer in self.__layers}\n",
    "  def __test_model(self, hps, test_batch, save_dir = 'params_all_models_5_days.pkl'):\n",
    "    # set up global objects for pickling circuit execution results\n",
    "    global global_number_of_circuits_executed\n",
    "    global global_hardware_run_results_dict\n",
    "    global_number_of_circuits_executed = 0\n",
    "    global_hardware_run_results_dict = {\n",
    "        'model_type' : hps.model_type,\n",
    "        'measurementRes' : None,\n",
    "        'epsilon' : hps.epsilon,\n",
    "        'backend_name' : None,\n",
    "        'layer_type' : hps.layer_type,\n",
    "    }\n",
    "    if hps.layer_type in ['linear','linear_svb']:\n",
    "      layer_func = linear\n",
    "    elif hps.layer_type=='ortho':\n",
    "      layer_func = ortho_linear\n",
    "    elif hps.layer_type=='noisy_ortho':\n",
    "      layer_func = partial(ortho_linear_noisy,noise_scale=0.01)\n",
    "    elif hps.layer_type=='hardware_ortho':\n",
    "      # TODO want to run this on Quantinuum device \n",
    "      layer_func = ortho_linear_hardware\n",
    "\n",
    "    if hps.model_type == 'simple':\n",
    "      net = simple_network(hps=hps, layer_func=layer_func)\n",
    "    elif hps.model_type == 'recurrent':\n",
    "      net = recurrent_network(hps=hps, layer_func=layer_func)\n",
    "    elif hps.model_type == 'lstm':\n",
    "      net = lstm_network(hps=hps, layer_func=layer_func)\n",
    "    elif hps.model_type == 'attention':\n",
    "      net = attention_network(hps=hps, layer_func=layer_func)\n",
    "    \n",
    "    opt = optax.adam(1E-3)\n",
    "    key, init_key = jax.random.split(self.__key)\n",
    "    _, state, _ = net.init(init_key, (1, hps.n_steps, 1))\n",
    "    loss_metric = entropy_loss\n",
    "\n",
    "    # Training\n",
    "\n",
    "    train_fn, loss_fn = build_train_fn(hps, net, opt, loss_metric)\n",
    "\n",
    "    train_info = load_params(save_dir)\n",
    "    layer_type = \"noisy_ortho\" if hps.layer_type == 'hardware_ortho' else hps.layer_type\n",
    "    train_losses, params = train_info[layer_type][str(hps.epsilon)][hps.model_type]\n",
    "    loss, _ = loss_fn(params, state, key, test_batch[...,None])\n",
    "    print(f'Model = {hps.model_type} | Layer = {hps.layer_type} | EPS = {hps.epsilon}| Loss = {loss} | #circs = {global_number_of_circuits_executed}')\n",
    "    return loss\n",
    "  def test(self, inputs):\n",
    "    for model in self.__models:\n",
    "      for eps in self.__eps:\n",
    "        for layer in self.__layers:\n",
    "            hps = HyperParams(S0=100,\n",
    "                  n_steps=5,\n",
    "                  n_paths=120000,\n",
    "                  discrete_path=True,\n",
    "                  strike_price=100,\n",
    "                  epsilon=eps,\n",
    "                  sigma=0.2,\n",
    "                  risk_free=0,\n",
    "                  dividend=0,\n",
    "                  model_type=model,\n",
    "                  layer_type=layer,\n",
    "                  n_features=8,\n",
    "                  n_layers=1,\n",
    "                  loss_param=1.0,\n",
    "                  batch_size=5,\n",
    "                  test_size=0.2,\n",
    "                  optimizer='adam',\n",
    "                  learning_rate=1E-3,\n",
    "                  num_epochs=100)\n",
    "            self.test_info[layer][str(eps)][model] = self.__test_model(hps, inputs)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T-RtLEOMgIHt"
   },
   "outputs": [],
   "source": [
    "seed = 100\n",
    "key = jax.random.PRNGKey(seed)\n",
    "\n",
    "# LAYERS = ['hardware_ortho']\n",
    "# EPS = [ 0.01]\n",
    "# MODELS = ['simple','recurrent','lstm','attention']\n",
    "\n",
    "# LAYERS = ['linear','ortho','noisy_ortho','hardware_ortho']\n",
    "# EPS = [ 0.01]\n",
    "# MODELS = ['lstm']\n",
    "\n",
    "# test only\n",
    "\n",
    "LAYERS = ['hardware_ortho']\n",
    "EPS = [ 0.01]\n",
    "MODELS = ['simple','recurrent']\n",
    "\n",
    "\n",
    "dhb = DeepHedgingBenchmark(key=key,eps=EPS, layers=LAYERS, models=MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSgwaeMQIcjF",
    "outputId": "0c48b882-16ea-498d-d68b-656381802006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precomputed counts from data/1031_simple_quantinuum_H1-2E_hardware_ortho_0.01.json\n",
      "Model = simple | Layer = hardware_ortho | EPS = 0.01| Loss = 2.0746476650238037 | #circs = 25\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Model recurrent not supported yet, only simple model is supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdhb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 83\u001b[0m, in \u001b[0;36mDeepHedgingBenchmark.test\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__layers:\n\u001b[1;32m     64\u001b[0m     hps \u001b[38;5;241m=\u001b[39m HyperParams(S0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     65\u001b[0m           n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     66\u001b[0m           n_paths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m           learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1E-3\u001b[39m,\n\u001b[1;32m     82\u001b[0m           num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_info[layer][\u001b[38;5;28mstr\u001b[39m(eps)][model] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__test_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 57\u001b[0m, in \u001b[0;36mDeepHedgingBenchmark.__test_model\u001b[0;34m(self, hps, test_batch, save_dir)\u001b[0m\n\u001b[1;32m     55\u001b[0m layer_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoisy_ortho\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hps\u001b[38;5;241m.\u001b[39mlayer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhardware_ortho\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m hps\u001b[38;5;241m.\u001b[39mlayer_type\n\u001b[1;32m     56\u001b[0m train_losses, params \u001b[38;5;241m=\u001b[39m train_info[layer_type][\u001b[38;5;28mstr\u001b[39m(hps\u001b[38;5;241m.\u001b[39mepsilon)][hps\u001b[38;5;241m.\u001b[39mmodel_type]\n\u001b[0;32m---> 57\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhps\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Layer = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhps\u001b[38;5;241m.\u001b[39mlayer_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | EPS = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhps\u001b[38;5;241m.\u001b[39mepsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m| Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | #circs = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_number_of_circuits_executed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/omniai/work/instance1/jupyter/jpmc-qcware-deephedging/train.py:22\u001b[0m, in \u001b[0;36mbuild_train_fn.<locals>.loss_fn\u001b[0;34m(params, state, key, inputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     I \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(inputs \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m outputs, state \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate( \n\u001b[1;32m     24\u001b[0m     (\n\u001b[1;32m     25\u001b[0m         outputs,\n\u001b[1;32m     26\u001b[0m      jnp\u001b[38;5;241m.\u001b[39mzeros_like(outputs[:, [\u001b[38;5;241m0\u001b[39m], :])\n\u001b[1;32m     27\u001b[0m ), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m deltas \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m     30\u001b[0m     (\n\u001b[1;32m     31\u001b[0m         outputs[:, [\u001b[38;5;241m0\u001b[39m], :],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     35\u001b[0m )\n",
      "Cell \u001b[0;32mIn [7], line 53\u001b[0m, in \u001b[0;36mrecurrent_network.<locals>.apply_fn\u001b[0;34m(params, state, key, inputs)\u001b[0m\n\u001b[1;32m     51\u001b[0m prev_state \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     52\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m _, outputs \u001b[38;5;241m=\u001b[39m \u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m outputs \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqueeze(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn [7], line 20\u001b[0m, in \u001b[0;36mscan\u001b[0;34m(f, init, xs, length)\u001b[0m\n\u001b[1;32m     18\u001b[0m ys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs:\n\u001b[0;32m---> 20\u001b[0m   carry, y \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m   ys\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m carry, jnp\u001b[38;5;241m.\u001b[39mstack(ys)\n",
      "Cell \u001b[0;32mIn [7], line 47\u001b[0m, in \u001b[0;36mrecurrent_network.<locals>.apply_fn.<locals>.cell_fn\u001b[0;34m(prev_outputs, inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m inp \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m     46\u001b[0m inp \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate([prev_outputs, inp], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# print(f'inputs shape = {inp.shape} deltas shape = {delta.shape}')\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m delta, delta\n",
      "File \u001b[0;32m/opt/omniai/work/instance1/jupyter/jpmc-qcware-deephedging/qnn.py:263\u001b[0m, in \u001b[0;36msequential.<locals>.apply_fn\u001b[0;34m(params, state, key, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39minit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m         outputs, new_module_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m         new_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx)] \u001b[38;5;241m=\u001b[39m new_module_state\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [6], line 55\u001b[0m, in \u001b[0;36mortho_linear_hardware.<locals>.apply_fn\u001b[0;34m(params, state, key, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     circs\u001b[38;5;241m.\u001b[39mappend(prepare_circuit(\u001b[38;5;28minput\u001b[39m,params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# run circuits and truncate to desired number of outputs\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m outputs \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mrun_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircs\u001b[49m\u001b[43m)\u001b[49m)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39mn_features:]\n\u001b[1;32m     57\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mreshape(out_shape)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# unitary = make_unitary(params['t'])\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# outputs = jnp.dot(inputs, unitary.T)[..., -n_features:]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# outputs = inputs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [5], line 132\u001b[0m, in \u001b[0;36mrun_circuit\u001b[0;34m(circs, backend_name)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytket\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantinuum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuantinuumBackend\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_hardware_run_results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_hardware_run_results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported yet, only simple model is supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m outpath_stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1031_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_hardware_run_results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_hardware_run_results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_hardware_run_results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m outpath_result_final \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutpath_stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Model recurrent not supported yet, only simple model is supported."
     ]
    }
   ],
   "source": [
    "dhb.test(test_batch)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "qcware_env",
   "language": "python",
   "name": "qcware_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "aeebd5b777ec7291cb611112c8ba5d8721451214eea05e9ccddb5913c09817ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
